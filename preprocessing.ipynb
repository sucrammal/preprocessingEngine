{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d64c39c",
   "metadata": {},
   "source": [
    "# Burner Detection Preprocessing Engine\n",
    "\n",
    "**Pipeline Overview:** Download model → Process images → Evaluate burner classification\n",
    "\n",
    "**Key Features:**\n",
    "1. **Simple Evaluation**: Presence/absence of burners (binary classification)\n",
    "2. **Advanced Evaluation**: Spatial IoU-based matching (object detection metrics)  \n",
    "3. **Preprocessing Comparison**: Test different normalization techniques for lighting variations\n",
    "4. **Visualization Demo**: Visual walkthrough of pipeline on sample image\n",
    "\n",
    "**Preprocessing Methods Available:**\n",
    "- **Simple**: Standard 0-1 normalization (baseline)\n",
    "- **GCN**: Global Contrast Normalization (handles overall brightness differences)\n",
    "- **LCN**: Local Contrast Normalization (handles local lighting/shadow variations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9676a1c7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb09fc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import tensorflow as tf\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "VIAM_CONFIG = {\n",
    "    \"model_name\": os.getenv(\"VIAM_MODEL_NAME\", \"your-burner-detection-model\"),\n",
    "    \"model_org_id\": os.getenv(\"VIAM_MODEL_ORG_ID\", \"your-model-org-id\"),\n",
    "    \"model_version\": os.getenv(\"VIAM_MODEL_VERSION\", \"2024-XX-XXTXX-XX-XX\"),\n",
    "}\n",
    "\n",
    "METADATA_DIR = os.getenv(\"METADATA_DIR\", \"metadata\")\n",
    "IMAGES_DIR = os.getenv(\"IMAGES_DIR\", \"data\")\n",
    "MODEL_DIR = os.getenv(\"MODEL_DIR\", \"models\")\n",
    "\n",
    "print(f\"Model: {VIAM_CONFIG['model_name']} v{VIAM_CONFIG['model_version']}\")\n",
    "print(f\"Data: {len(glob.glob(os.path.join(METADATA_DIR, '*.json')))} metadata files\")\n",
    "\n",
    "# Quick preview of ground truth format\n",
    "metadata_files_preview = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "if metadata_files_preview:\n",
    "    print(f\"\\n📋 Sample ground truth format:\")\n",
    "    with open(metadata_files_preview[0], 'r') as f:\n",
    "        sample_metadata = json.load(f)\n",
    "    if 'annotations' in sample_metadata:\n",
    "        for bbox in sample_metadata['annotations'].get('bboxes', [])[:3]:  # Show first 3\n",
    "            print(f\"  - Label: '{bbox.get('label', 'N/A')}'\")\n",
    "    else:\n",
    "        print(\"  No annotations found in sample metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02f4ef",
   "metadata": {},
   "source": [
    "## Step 1: Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb30590",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def download_model():\n",
    "    \"\"\"Download TFLite model from Viam and extract if needed\"\"\"\n",
    "    import tarfile\n",
    "    \n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    cmd = [\n",
    "        \"viam\", \"packages\", \"export\",\n",
    "        \"--org-id\", VIAM_CONFIG[\"model_org_id\"],\n",
    "        \"--name\", VIAM_CONFIG[\"model_name\"],\n",
    "        \"--version\", VIAM_CONFIG[\"model_version\"],\n",
    "        \"--type\", \"ml_model\",\n",
    "        \"--destination\", MODEL_DIR\n",
    "    ]\n",
    "    \n",
    "    print(\"Downloading model...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ Model downloaded successfully\")\n",
    "        \n",
    "        # First, check if .tflite file already exists (already extracted)\n",
    "        tflite_files = glob.glob(os.path.join(MODEL_DIR, \"**/*.tflite\"), recursive=True)\n",
    "        if tflite_files:\n",
    "            model_path = tflite_files[0]\n",
    "            print(f\"✅ Model found: {model_path}\")\n",
    "            return model_path\n",
    "        \n",
    "        # If no .tflite file found, look for .tar.gz files to extract\n",
    "        tar_files = glob.glob(os.path.join(MODEL_DIR, \"**/*.tar.gz\"), recursive=True)\n",
    "        if tar_files:\n",
    "            for tar_file in tar_files:\n",
    "                print(f\"📦 Extracting {tar_file}...\")\n",
    "                try:\n",
    "                    with tarfile.open(tar_file, 'r:gz') as tar:\n",
    "                        tar.extractall(os.path.dirname(tar_file))\n",
    "                    print(f\"✅ Extracted {tar_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error extracting {tar_file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Now look for .tflite files after extraction\n",
    "            tflite_files = glob.glob(os.path.join(MODEL_DIR, \"**/*.tflite\"), recursive=True)\n",
    "            if tflite_files:\n",
    "                model_path = tflite_files[0]\n",
    "                print(f\"✅ Model ready: {model_path}\")\n",
    "                return model_path\n",
    "            else:\n",
    "                print(\"❌ No .tflite file found after extraction\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"❌ No .tar.gz files found to extract\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"❌ Download failed: {result.stderr}\")\n",
    "        return None\n",
    "\n",
    "# Download model\n",
    "model_path = download_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86749d40",
   "metadata": {},
   "source": [
    "## Core Processing Functions\n",
    "\n",
    "These functions are used by both evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc7834",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_model(model_path: str):\n",
    "    \"\"\"Load TFLite model\"\"\"\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "def preprocess_image(image_path: str, target_size: tuple = (640, 640), input_dtype=np.float32, \n",
    "                    normalization_method: str = \"simple\") -> np.ndarray:\n",
    "    \"\"\"Preprocess image for model with different normalization techniques\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize(target_size)\n",
    "    image_array = np.array(image, dtype=np.float32)\n",
    "    \n",
    "    # Apply different normalization techniques\n",
    "    if normalization_method == \"simple\":\n",
    "        # Simple 0-1 normalization\n",
    "        normalized = image_array / 255.0\n",
    "    \n",
    "    elif normalization_method == \"gcn\":\n",
    "        # Global Contrast Normalization\n",
    "        normalized = apply_global_contrast_normalization(image_array)\n",
    "    \n",
    "    elif normalization_method == \"lcn\":\n",
    "        # Local Contrast Normalization\n",
    "        normalized = apply_local_contrast_normalization(image_array)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization method: {normalization_method}\")\n",
    "    \n",
    "    # Convert to model's expected dtype\n",
    "    if input_dtype == np.uint8:\n",
    "        # For quantized models, convert back to 0-255 range\n",
    "        normalized = np.clip(normalized * 255.0, 0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        # Keep as float32\n",
    "        normalized = normalized.astype(np.float32)\n",
    "    \n",
    "    return np.expand_dims(normalized, axis=0)\n",
    "\n",
    "def apply_global_contrast_normalization(image_array: np.ndarray, epsilon: float = 1e-8) -> np.ndarray:\n",
    "    \"\"\"Apply Global Contrast Normalization (GCN)\"\"\"\n",
    "    # Calculate global mean and std across all pixels and channels\n",
    "    global_mean = np.mean(image_array)\n",
    "    global_std = np.std(image_array)\n",
    "    \n",
    "    # Normalize: (X - μ) / (σ + ε)\n",
    "    normalized = (image_array - global_mean) / (global_std + epsilon)\n",
    "    \n",
    "    # Scale back to reasonable range (0-1)\n",
    "    normalized = (normalized - normalized.min()) / (normalized.max() - normalized.min() + epsilon)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def apply_local_contrast_normalization(image_array: np.ndarray, window_size: int = 9, epsilon: float = 1e-8) -> np.ndarray:\n",
    "    \"\"\"Apply Local Contrast Normalization (LCN)\"\"\"\n",
    "    from scipy import ndimage\n",
    "    \n",
    "    # Convert to grayscale for LCN calculation, then apply to all channels\n",
    "    normalized = np.zeros_like(image_array, dtype=np.float32)\n",
    "    \n",
    "    for channel in range(image_array.shape[2]):\n",
    "        channel_data = image_array[:, :, channel]\n",
    "        \n",
    "        # Calculate local mean using a uniform filter\n",
    "        local_mean = ndimage.uniform_filter(channel_data, size=window_size, mode='reflect')\n",
    "        \n",
    "        # Calculate local standard deviation\n",
    "        local_variance = ndimage.uniform_filter(channel_data**2, size=window_size, mode='reflect') - local_mean**2\n",
    "        local_std = np.sqrt(np.maximum(local_variance, 0)) + epsilon\n",
    "        \n",
    "        # Apply LCN: (X - local_mean) / (local_std + ε)\n",
    "        channel_normalized = (channel_data - local_mean) / local_std\n",
    "        \n",
    "        # Scale to 0-1 range\n",
    "        channel_normalized = (channel_normalized - channel_normalized.min()) / (channel_normalized.max() - channel_normalized.min() + epsilon)\n",
    "        \n",
    "        normalized[:, :, channel] = channel_normalized\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def run_inference(image_path: str, interpreter, normalization_method: str = \"simple\") -> List[Dict]:\n",
    "    \"\"\"Run inference and extract burner detections\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        return []\n",
    "    \n",
    "    # Get input/output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Preprocess image with correct dtype and normalization method\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "    target_size = (input_shape[1], input_shape[2])\n",
    "    image_data = preprocess_image(image_path, target_size, input_dtype, normalization_method)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], image_data)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get outputs\n",
    "    outputs = {}\n",
    "    for detail in output_details:\n",
    "        outputs[detail['name']] = interpreter.get_tensor(detail['index'])\n",
    "    \n",
    "    # Parse detections (adjust tensor names based on your model)\n",
    "    detections = []\n",
    "    boxes = outputs.get('detection_boxes', outputs.get('boxes', None))\n",
    "    classes = outputs.get('detection_classes', outputs.get('classes', None))\n",
    "    scores = outputs.get('detection_scores', outputs.get('scores', None))\n",
    "    \n",
    "    if boxes is not None and classes is not None and scores is not None:\n",
    "        # Remove batch dimension\n",
    "        if boxes.ndim == 3: boxes = boxes[0]\n",
    "        if classes.ndim == 2: classes = classes[0]\n",
    "        if scores.ndim == 2: scores = scores[0]\n",
    "        \n",
    "        for i, score in enumerate(scores):\n",
    "            if score > 0.5:  # Confidence threshold\n",
    "                detections.append({\n",
    "                    \"class_id\": int(classes[i]),\n",
    "                    \"confidence\": float(score),\n",
    "                    \"bbox\": [float(x) for x in boxes[i]]\n",
    "                })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def find_image_for_metadata(metadata_file: str) -> str:\n",
    "    \"\"\"Find corresponding image file for metadata\"\"\"\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    filename = metadata.get('fileName', '')\n",
    "    binary_id = metadata.get('id', '')\n",
    "    \n",
    "    # Try direct filename match\n",
    "    image_path = os.path.join(IMAGES_DIR, filename)\n",
    "    if os.path.exists(image_path):\n",
    "        return image_path\n",
    "    \n",
    "    # Try binary ID match\n",
    "    for file in os.listdir(IMAGES_DIR):\n",
    "        if binary_id in file and file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            return os.path.join(IMAGES_DIR, file)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_all_images(model_path: str):\n",
    "    \"\"\"Process all images and return results\"\"\"\n",
    "    if not model_path or not os.path.exists(model_path):\n",
    "        print(\"❌ Model not found\")\n",
    "        return []\n",
    "    \n",
    "    interpreter = load_model(model_path)\n",
    "    metadata_files = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(metadata_files)} images...\")\n",
    "    print(\"\\n=== Label Matching Logic ===\")\n",
    "    print(\"Ground Truth: Looking for labels containing 'burner' (case-insensitive)\")\n",
    "    print(\"Predictions: Looking for detections with class_id=0 (based on labels.txt)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, metadata_file in enumerate(metadata_files):\n",
    "        # Load metadata\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Find corresponding image\n",
    "        image_path = find_image_for_metadata(metadata_file)\n",
    "        if not image_path:\n",
    "            continue\n",
    "        \n",
    "        # Run inference\n",
    "        detections = run_inference(image_path, interpreter, \"simple\")\n",
    "        \n",
    "        # Extract ground truth burner labels\n",
    "        gt_burners = []\n",
    "        all_gt_labels = []\n",
    "        if 'annotations' in metadata:\n",
    "            for bbox in metadata['annotations'].get('bboxes', []):\n",
    "                label = bbox.get('label', '')\n",
    "                all_gt_labels.append(label)\n",
    "                if 'burner' in label.lower():\n",
    "                    gt_burners.append(label)\n",
    "        \n",
    "        # Check if model detected burners (class_id 0 = burner based on labels.txt)\n",
    "        pred_burners = [det for det in detections if det['class_id'] == 0]\n",
    "        \n",
    "        results.append({\n",
    "            \"file\": os.path.basename(metadata_file),\n",
    "            \"image_path\": image_path,\n",
    "            \"ground_truth_burners\": len(gt_burners),\n",
    "            \"predicted_burners\": len(pred_burners),\n",
    "            \"detections\": detections,\n",
    "            \"has_burner_gt\": len(gt_burners) > 0,\n",
    "            \"has_burner_pred\": len(pred_burners) > 0,\n",
    "            \"all_gt_labels\": all_gt_labels\n",
    "        })\n",
    "        \n",
    "        # Print detailed results for first image only\n",
    "        if i == 0:  # First image\n",
    "            filename = os.path.basename(image_path)\n",
    "            print(f\"\\n  📸 {filename}\")\n",
    "            print(f\"     Ground truth: {all_gt_labels} → {len(gt_burners)} burners\")\n",
    "            print(f\"     Predictions: {len(detections)} total detections\")\n",
    "            for j, det in enumerate(detections):\n",
    "                print(f\"       - Detection {j+1}: class_id={det['class_id']}, confidence={det['confidence']:.3f}\")\n",
    "            print(f\"     Result: GT={len(gt_burners)}, Pred={len(pred_burners)}\")\n",
    "        elif (i + 1) % 500 == 0:\n",
    "            # Progress update every 500 images\n",
    "            print(f\"  Progress: {i + 1}/{len(metadata_files)} images processed\")\n",
    "        \n",
    "                 # No individual image output for images 2-8000+\n",
    "    \n",
    "    print(f\"\\n✅ Processing complete: {len(results)} images processed\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9465f",
   "metadata": {},
   "source": [
    "## Advanced Evaluation with IoU Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b842a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def convert_bbox_format(bbox, format_type: str) -> List[float]:\n",
    "    \"\"\"Convert between different bounding box formats\"\"\"\n",
    "    if format_type == \"gt_to_pred\":\n",
    "        # Ground truth: {xMinNormalized, yMinNormalized, xMaxNormalized, yMaxNormalized}\n",
    "        # to Model: [ymin, xmin, ymax, xmax]\n",
    "        return [bbox[\"yMinNormalized\"], bbox[\"xMinNormalized\"], bbox[\"yMaxNormalized\"], bbox[\"xMaxNormalized\"]]\n",
    "    elif format_type == \"pred_to_gt\":\n",
    "        # Model: [ymin, xmin, ymax, xmax]\n",
    "        # to Ground truth format: {xMinNormalized, yMinNormalized, xMaxNormalized, yMaxNormalized}\n",
    "        return {\n",
    "            \"xMinNormalized\": bbox[1],\n",
    "            \"yMinNormalized\": bbox[0],\n",
    "            \"xMaxNormalized\": bbox[3],\n",
    "            \"yMaxNormalized\": bbox[2]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown format_type: {format_type}\")\n",
    "\n",
    "def calculate_iou(box1: List[float], box2: List[float]) -> float:\n",
    "    \"\"\"Calculate IoU between two bounding boxes in [ymin, xmin, ymax, xmax] format\"\"\"\n",
    "    # box1 and box2 should be in format [ymin, xmin, ymax, xmax]\n",
    "    y1_min, x1_min, y1_max, x1_max = box1\n",
    "    y2_min, x2_min, y2_max, x2_max = box2\n",
    "    \n",
    "    # Calculate intersection\n",
    "    x_left = max(x1_min, x2_min)\n",
    "    y_top = max(y1_min, y2_min)\n",
    "    x_right = min(x1_max, x2_max)\n",
    "    y_bottom = min(y1_max, y2_max)\n",
    "    \n",
    "    if x_right <= x_left or y_bottom <= y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
    "    \n",
    "    # Calculate union\n",
    "    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def evaluate_with_iou(results: List[Dict], iou_threshold: float = 0.5) -> Dict:\n",
    "    \"\"\"Evaluate predictions using IoU matching\"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    detailed_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        if not result[\"inference_success\"]:\n",
    "            continue\n",
    "            \n",
    "        # Get ground truth burner boxes\n",
    "        gt_boxes = []\n",
    "        if 'annotations' in result.get('metadata', {}):\n",
    "            for bbox in result['metadata']['annotations'].get('bboxes', []):\n",
    "                if 'burner' in bbox.get('label', '').lower():\n",
    "                    gt_boxes.append(convert_bbox_format(bbox, \"gt_to_pred\"))\n",
    "        \n",
    "        # Get predicted burner boxes\n",
    "        pred_boxes = []\n",
    "        for det in result.get('detections', []):\n",
    "            if det['class_id'] == 0:  # burner class\n",
    "                pred_boxes.append(det['bbox'])\n",
    "        \n",
    "        # Match predictions to ground truth\n",
    "        matched_gt = set()\n",
    "        matched_pred = set()\n",
    "        matches = []\n",
    "        \n",
    "        for pred_idx, pred_box in enumerate(pred_boxes):\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "            \n",
    "            for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                if gt_idx in matched_gt:\n",
    "                    continue\n",
    "                    \n",
    "                iou = calculate_iou(pred_box, gt_box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = gt_idx\n",
    "            \n",
    "            if best_iou >= iou_threshold:\n",
    "                matched_gt.add(best_gt_idx)\n",
    "                matched_pred.add(pred_idx)\n",
    "                matches.append({\n",
    "                    'gt_idx': best_gt_idx,\n",
    "                    'pred_idx': pred_idx,\n",
    "                    'iou': best_iou,\n",
    "                    'confidence': result['detections'][pred_idx]['confidence']\n",
    "                })\n",
    "                true_positives += 1\n",
    "        \n",
    "        # Count false positives and false negatives\n",
    "        false_positives += len(pred_boxes) - len(matched_pred)\n",
    "        false_negatives += len(gt_boxes) - len(matched_gt)\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'file': result['file'],\n",
    "            'gt_boxes': len(gt_boxes),\n",
    "            'pred_boxes': len(pred_boxes),\n",
    "            'matches': matches,\n",
    "            'tp': len(matches),\n",
    "            'fp': len(pred_boxes) - len(matched_pred),\n",
    "            'fn': len(gt_boxes) - len(matched_gt)\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives,\n",
    "        'detailed_results': detailed_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd4940",
   "metadata": {},
   "source": [
    "## Visualization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd802c3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_single_image_pipeline(metadata_file: str, model_path: str):\n",
    "    \"\"\"Demonstrate the pipeline on a single sample image\"\"\"\n",
    "    \n",
    "    print(\"🔍 Loading sample image and metadata...\")\n",
    "    \n",
    "    # Load metadata and find image\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    image_path = find_image_for_metadata(metadata_file)\n",
    "    if not image_path:\n",
    "        print(f\"❌ No image found for {metadata_file}\")\n",
    "        return\n",
    "    \n",
    "    # Load model\n",
    "    interpreter = load_model(model_path)\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Step 1: Show ground truth\n",
    "    print(f\"\\n📋 Step 1: Ground Truth Analysis\")\n",
    "    print(f\"   Image: {os.path.basename(image_path)}\")\n",
    "    print(f\"   Original size: {original_image.size}\")\n",
    "    \n",
    "    all_gt_labels = []\n",
    "    gt_boxes = []\n",
    "    if 'annotations' in metadata:\n",
    "        for bbox in metadata['annotations'].get('bboxes', []):\n",
    "            label = bbox.get('label', '')\n",
    "            all_gt_labels.append(label)\n",
    "            if 'burner' in label.lower():\n",
    "                gt_boxes.append(bbox)\n",
    "    \n",
    "    print(f\"   All ground truth labels: {all_gt_labels}\")\n",
    "    print(f\"   Burner labels found: {len(gt_boxes)} burners\")\n",
    "    \n",
    "    # Step 2: Show preprocessing\n",
    "    print(f\"\\n🔄 Step 2: Preprocessing\")\n",
    "    input_details = interpreter.get_input_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "    target_size = (input_shape[1], input_shape[2])\n",
    "    \n",
    "    print(f\"   Model expects: {target_size} pixels, {input_dtype} data type\")\n",
    "    preprocessed = preprocess_image(image_path, target_size, input_dtype)\n",
    "    print(f\"   Preprocessed shape: {preprocessed.shape}\")\n",
    "    \n",
    "    # Step 3: Show inference results\n",
    "    print(f\"\\n🎯 Step 3: Inference Results\")\n",
    "    detections = run_inference(image_path, interpreter, \"simple\")\n",
    "    pred_boxes = [det for det in detections if det['class_id'] == 0]\n",
    "    \n",
    "    print(f\"   Total detections: {len(detections)}\")\n",
    "    print(f\"   Burner detections: {len(pred_boxes)}\")\n",
    "    \n",
    "    for i, det in enumerate(pred_boxes):\n",
    "        print(f\"     - Burner {i+1}: confidence={det['confidence']:.3f}, bbox={det['bbox']}\")\n",
    "    \n",
    "    # Step 4: Visual comparison\n",
    "    print(f\"\\n📸 Step 4: Creating Visual Comparison\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Original + Ground Truth\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(f\"Ground Truth\\n({len(gt_boxes)} burners)\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    for bbox in gt_boxes:\n",
    "        w, h = original_image.size\n",
    "        x_min = int(bbox[\"xMinNormalized\"] * w)\n",
    "        y_min = int(bbox[\"yMinNormalized\"] * h)\n",
    "        x_max = int(bbox[\"xMaxNormalized\"] * w)\n",
    "        y_max = int(bbox[\"yMaxNormalized\"] * h)\n",
    "        \n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                           fill=False, edgecolor='green', linewidth=3)\n",
    "        axes[0].add_patch(rect)\n",
    "        axes[0].text(x_min, y_min - 10, \"GT\", color='green', fontsize=12, weight='bold')\n",
    "    \n",
    "    # Preprocessed image\n",
    "    if input_dtype == np.uint8:\n",
    "        display_image = preprocessed[0].astype(np.uint8)\n",
    "    else:\n",
    "        display_image = (preprocessed[0] * 255).astype(np.uint8)\n",
    "    \n",
    "    axes[1].imshow(display_image)\n",
    "    axes[1].set_title(f\"Preprocessed\\n{target_size}, {input_dtype}\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Original + Predictions\n",
    "    axes[2].imshow(original_image)\n",
    "    axes[2].set_title(f\"Predictions\\n({len(pred_boxes)} burners)\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    for i, det in enumerate(pred_boxes):\n",
    "        w, h = original_image.size\n",
    "        y_min = int(det['bbox'][0] * h)\n",
    "        x_min = int(det['bbox'][1] * w)\n",
    "        y_max = int(det['bbox'][2] * h)\n",
    "        x_max = int(det['bbox'][3] * w)\n",
    "        \n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                           fill=False, edgecolor='red', linewidth=3)\n",
    "        axes[2].add_patch(rect)\n",
    "        axes[2].text(x_min, y_min - 10, f\"{det['confidence']:.2f}\", \n",
    "                    color='red', fontsize=12, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"pipeline_demo.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n✅ Pipeline Demo Complete!\")\n",
    "    print(f\"   📊 Ground Truth: {len(gt_boxes)} burners\")\n",
    "    print(f\"   🎯 Predictions: {len(pred_boxes)} burners\")\n",
    "    print(f\"   📸 Visualization saved: pipeline_demo.png\")\n",
    "    print(f\"   🎪 Demo shows: GT (green) vs Predictions (red)\")\n",
    "    \n",
    "    if len(gt_boxes) > 0 and len(pred_boxes) > 0:\n",
    "        print(f\"   ✅ Model found burners in image with burners!\")\n",
    "    elif len(gt_boxes) > 0 and len(pred_boxes) == 0:\n",
    "        print(f\"   ❌ Model missed burners that should be there\")\n",
    "    elif len(gt_boxes) == 0 and len(pred_boxes) > 0:\n",
    "        print(f\"   ❌ Model detected burners where there are none\")\n",
    "    else:\n",
    "        print(f\"   ✅ Model correctly found no burners in image with no burners\")\n",
    "\n",
    "def process_all_images_with_iou(model_path: str):\n",
    "    \"\"\"Process all images with full metadata for IoU evaluation\"\"\"\n",
    "    if not model_path or not os.path.exists(model_path):\n",
    "        print(\"❌ Model not found\")\n",
    "        return []\n",
    "    \n",
    "    interpreter = load_model(model_path)\n",
    "    metadata_files = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(metadata_files)} images for IoU evaluation...\")\n",
    "    \n",
    "    for i, metadata_file in enumerate(metadata_files):\n",
    "        # Load metadata\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Find corresponding image\n",
    "        image_path = find_image_for_metadata(metadata_file)\n",
    "        if not image_path:\n",
    "            continue\n",
    "        \n",
    "        # Run inference\n",
    "        detections = run_inference(image_path, interpreter, \"simple\")\n",
    "        \n",
    "        results.append({\n",
    "            \"file\": os.path.basename(metadata_file),\n",
    "            \"image_path\": image_path,\n",
    "            \"metadata\": metadata,\n",
    "            \"detections\": detections,\n",
    "            \"inference_success\": True\n",
    "        })\n",
    "        \n",
    "        # Progress update every 500 images\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"  Progress: {i + 1}/{len(metadata_files)} images processed\")\n",
    "    \n",
    "    print(f\"\\n✅ IoU evaluation complete: {len(results)} images processed\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe5704d",
   "metadata": {},
   "source": [
    "## Evaluation Methods\n",
    "\n",
    "The notebook provides two independent evaluation approaches:\n",
    "\n",
    "**Method 1: Simple Evaluation**\n",
    "- Binary classification: \"Does the image contain burners?\"\n",
    "- Counts burner instances, compares presence/absence\n",
    "- Fast, good for overall accuracy assessment\n",
    "\n",
    "**Method 2: Advanced IoU Evaluation** \n",
    "- Object detection metrics with spatial matching\n",
    "- Uses IoU thresholds to match predictions to ground truth\n",
    "- Provides precision, recall, F1 scores\n",
    "- More thorough, accounts for spatial accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2c5c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Process all images\n",
    "if model_path:\n",
    "    # ===== EVALUATION METHOD 1: Simple Presence/Absence =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 SIMPLE EVALUATION (Presence/Absence)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = process_all_images(model_path)\n",
    "    \n",
    "    # ===== EVALUATION METHOD 2: Advanced IoU-Based =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 ADVANCED EVALUATION (IoU-Based)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    iou_results = process_all_images_with_iou(model_path)\n",
    "    if iou_results:\n",
    "        evaluation = evaluate_with_iou(iou_results, iou_threshold=0.5)\n",
    "        \n",
    "        print(f\"\\n📊 IoU Evaluation Results (threshold=0.5):\")\n",
    "        print(f\"   Precision: {evaluation['precision']:.3f}\")\n",
    "        print(f\"   Recall: {evaluation['recall']:.3f}\")\n",
    "        print(f\"   F1 Score: {evaluation['f1']:.3f}\")\n",
    "        print(f\"   True Positives: {evaluation['true_positives']}\")\n",
    "        print(f\"   False Positives: {evaluation['false_positives']}\")\n",
    "        print(f\"   False Negatives: {evaluation['false_negatives']}\")\n",
    "        \n",
    "        # Show detailed per-image results\n",
    "        print(f\"\\n📋 Per-Image Results:\")\n",
    "        for detail in evaluation['detailed_results'][:5]:  # Show first 5\n",
    "            print(f\"   {detail['file']}: GT={detail['gt_boxes']}, Pred={detail['pred_boxes']}, \"\n",
    "                  f\"TP={detail['tp']}, FP={detail['fp']}, FN={detail['fn']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Skipping processing - no model available\")\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c84776",
   "metadata": {},
   "source": [
    "## Step 3: Check Burner Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b56aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_burner_performance(results: List[Dict]):\n",
    "    \"\"\"Analyze burner detection performance\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    total = len(results)\n",
    "    true_positives = sum(1 for r in results if r['has_burner_gt'] and r['has_burner_pred'])\n",
    "    false_positives = sum(1 for r in results if not r['has_burner_gt'] and r['has_burner_pred'])\n",
    "    false_negatives = sum(1 for r in results if r['has_burner_gt'] and not r['has_burner_pred'])\n",
    "    true_negatives = sum(1 for r in results if not r['has_burner_gt'] and not r['has_burner_pred'])\n",
    "    \n",
    "    accuracy = (true_positives + true_negatives) / total if total > 0 else 0\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    print(\"\\n=== Simple Burner Classification Results ===\")\n",
    "    print(f\"Total images: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Precision: {precision:.2%}\")\n",
    "    print(f\"Recall: {recall:.2%}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  True Positives: {true_positives}\")\n",
    "    print(f\"  False Positives: {false_positives}\")\n",
    "    print(f\"  False Negatives: {false_negatives}\")\n",
    "    print(f\"  True Negatives: {true_negatives}\")\n",
    "    \n",
    "    # Show misclassified examples\n",
    "    print(f\"\\nMisclassifications:\")\n",
    "    for result in results:\n",
    "        if result['has_burner_gt'] != result['has_burner_pred']:\n",
    "            status = \"Missing burner\" if result['has_burner_gt'] else \"False detection\"\n",
    "            print(f\"  {result['file']}: {status}\")\n",
    "\n",
    "# Analyze results\n",
    "analyze_burner_performance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005fede",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e05802",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    output_file = \"burner_classification_results.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\n📄 Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5c424",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ===== PREPROCESSING METHOD COMPARISON =====\n",
    "if model_path:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🧪 PREPROCESSING COMPARISON EXPERIMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test preprocessing methods on subset of images\n",
    "    comparison_results = compare_preprocessing_methods(model_path, max_images=100)\n",
    "    \n",
    "    # Visualize preprocessing effects on a sample image\n",
    "    metadata_files = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "    if metadata_files:\n",
    "        sample_metadata = metadata_files[0]\n",
    "        print(f\"\\nVisualizing preprocessing effects on: {os.path.basename(sample_metadata)}\")\n",
    "        visualize_preprocessing_effects(sample_metadata, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ae55c",
   "metadata": {},
   "source": [
    "## Preprocessing Method Comparison\n",
    "\n",
    "Compare different normalization techniques to handle lighting variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5fa39",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_images_with_preprocessing_method(model_path: str, normalization_method: str, \n",
    "                                           max_images: int = 100) -> List[Dict]:\n",
    "    \"\"\"Process subset of images with specific preprocessing method\"\"\"\n",
    "    if not model_path or not os.path.exists(model_path):\n",
    "        print(f\"❌ Model not found\")\n",
    "        return []\n",
    "    \n",
    "    interpreter = load_model(model_path)\n",
    "    metadata_files = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "    \n",
    "    # Limit to subset for comparison (processing all 8000+ would take too long)\n",
    "    test_files = metadata_files[:max_images]\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Testing {normalization_method} normalization on {len(test_files)} images...\")\n",
    "    \n",
    "    for i, metadata_file in enumerate(test_files):\n",
    "        # Load metadata\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Find corresponding image\n",
    "        image_path = find_image_for_metadata(metadata_file)\n",
    "        if not image_path:\n",
    "            continue\n",
    "        \n",
    "        # Run inference with specific normalization method\n",
    "        detections = run_inference(image_path, interpreter, normalization_method)\n",
    "        \n",
    "        # Extract ground truth burner labels\n",
    "        gt_burners = []\n",
    "        if 'annotations' in metadata:\n",
    "            for bbox in metadata['annotations'].get('bboxes', []):\n",
    "                if 'burner' in bbox.get('label', '').lower():\n",
    "                    gt_burners.append(bbox['label'])\n",
    "        \n",
    "        # Check if model detected burners\n",
    "        pred_burners = [det for det in detections if det['class_id'] == 0]\n",
    "        \n",
    "        results.append({\n",
    "            \"file\": os.path.basename(metadata_file),\n",
    "            \"image_path\": image_path,\n",
    "            \"ground_truth_burners\": len(gt_burners),\n",
    "            \"predicted_burners\": len(pred_burners),\n",
    "            \"detections\": detections,\n",
    "            \"has_burner_gt\": len(gt_burners) > 0,\n",
    "            \"has_burner_pred\": len(pred_burners) > 0,\n",
    "            \"normalization_method\": normalization_method\n",
    "        })\n",
    "        \n",
    "        # Progress update every 25 images for smaller batches\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f\"  Progress: {i + 1}/{len(test_files)} images processed\")\n",
    "    \n",
    "    print(f\"✅ {normalization_method} processing complete: {len(results)} images\")\n",
    "    return results\n",
    "\n",
    "def compare_preprocessing_methods(model_path: str, max_images: int = 100):\n",
    "    \"\"\"Compare all three preprocessing methods\"\"\"\n",
    "    methods = [\"simple\", \"gcn\", \"lcn\"]\n",
    "    all_results = {}\n",
    "    \n",
    "    print(\"🧪 PREPROCESSING METHOD COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Testing {len(methods)} normalization methods on {max_images} images each\")\n",
    "    print(\"Methods: Simple (0-1), GCN (Global Contrast), LCN (Local Contrast)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test each method\n",
    "    for method in methods:\n",
    "        print(f\"\\n🔬 Testing {method.upper()} normalization...\")\n",
    "        results = process_images_with_preprocessing_method(model_path, method, max_images)\n",
    "        all_results[method] = results\n",
    "    \n",
    "    # Compare results\n",
    "    print(f\"\\n📊 PREPROCESSING COMPARISON RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for method, results in all_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        total = len(results)\n",
    "        true_positives = sum(1 for r in results if r['has_burner_gt'] and r['has_burner_pred'])\n",
    "        false_positives = sum(1 for r in results if not r['has_burner_gt'] and r['has_burner_pred'])\n",
    "        false_negatives = sum(1 for r in results if r['has_burner_gt'] and not r['has_burner_pred'])\n",
    "        true_negatives = sum(1 for r in results if not r['has_burner_gt'] and not r['has_burner_pred'])\n",
    "        \n",
    "        accuracy = (true_positives + true_negatives) / total if total > 0 else 0\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Count total detections (regardless of correctness)\n",
    "        total_detections = sum(r['predicted_burners'] for r in results)\n",
    "        avg_detections = total_detections / total if total > 0 else 0\n",
    "        \n",
    "        print(f\"\\n🔸 {method.upper()} Normalization:\")\n",
    "        print(f\"   Accuracy:  {accuracy:.3f}\")\n",
    "        print(f\"   Precision: {precision:.3f}\")\n",
    "        print(f\"   Recall:    {recall:.3f}\")\n",
    "        print(f\"   F1 Score:  {f1:.3f}\")\n",
    "        print(f\"   Avg Detections/Image: {avg_detections:.2f}\")\n",
    "        print(f\"   TP: {true_positives}, FP: {false_positives}, FN: {false_negatives}, TN: {true_negatives}\")\n",
    "    \n",
    "    # Find best method\n",
    "    best_method = None\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for method, results in all_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "        total = len(results)\n",
    "        true_positives = sum(1 for r in results if r['has_burner_gt'] and r['has_burner_pred'])\n",
    "        false_positives = sum(1 for r in results if not r['has_burner_gt'] and r['has_burner_pred'])\n",
    "        false_negatives = sum(1 for r in results if r['has_burner_gt'] and not r['has_burner_pred'])\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_method = method\n",
    "    \n",
    "    if best_method:\n",
    "        print(f\"\\n🏆 BEST PREPROCESSING METHOD: {best_method.upper()}\")\n",
    "        print(f\"   Best F1 Score: {best_f1:.3f}\")\n",
    "        print(f\"   💡 Recommended for production use!\")\n",
    "    \n",
    "    # Save comparison results\n",
    "    comparison_file = \"preprocessing_comparison.json\"\n",
    "    with open(comparison_file, 'w') as f:\n",
    "        # Convert results to JSON-serializable format\n",
    "        serializable_results = {}\n",
    "        for method, results in all_results.items():\n",
    "            serializable_results[method] = []\n",
    "            for result in results:\n",
    "                # Remove non-serializable items\n",
    "                clean_result = {k: v for k, v in result.items() if k != 'detections'}\n",
    "                clean_result['detection_count'] = len(result.get('detections', []))\n",
    "                serializable_results[method].append(clean_result)\n",
    "        \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n📄 Comparison results saved to {comparison_file}\")\n",
    "    return all_results\n",
    "\n",
    "def visualize_preprocessing_effects(metadata_file: str, model_path: str):\n",
    "    \"\"\"Visualize the effects of different preprocessing methods on a single image\"\"\"\n",
    "    \n",
    "    print(\"🔍 Visualizing preprocessing effects on sample image...\")\n",
    "    \n",
    "    # Load metadata and find image\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    image_path = find_image_for_metadata(metadata_file)\n",
    "    if not image_path:\n",
    "        print(f\"❌ No image found for {metadata_file}\")\n",
    "        return\n",
    "    \n",
    "    # Load original image\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Test all three preprocessing methods\n",
    "    methods = [\"simple\", \"gcn\", \"lcn\"]\n",
    "    method_names = [\"Simple (0-1)\", \"Global Contrast Norm\", \"Local Contrast Norm\"]\n",
    "    \n",
    "    # Load model to get expected input format\n",
    "    interpreter = load_model(model_path)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "    target_size = (input_shape[1], input_shape[2])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Apply each preprocessing method\n",
    "    for i, (method, name) in enumerate(zip(methods, method_names)):\n",
    "        preprocessed = preprocess_image(image_path, target_size, input_dtype, method)\n",
    "        \n",
    "        # Convert back to display format\n",
    "        if input_dtype == np.uint8:\n",
    "            display_image = preprocessed[0].astype(np.uint8)\n",
    "        else:\n",
    "            display_image = (preprocessed[0] * 255).astype(np.uint8)\n",
    "        \n",
    "        axes[i + 1].imshow(display_image)\n",
    "        axes[i + 1].set_title(f\"{name}\")\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"preprocessing_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"📸 Preprocessing comparison saved: preprocessing_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cdcd0",
   "metadata": {},
   "source": [
    "## Pipeline Visualization Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PIPELINE DEMO: Visual Walkthrough =====\n",
    "if model_path:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📸 PIPELINE VISUALIZATION DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Visualize one sample image to demonstrate the pipeline\n",
    "    metadata_files = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "    if metadata_files:\n",
    "        sample_metadata = metadata_files[0]  # Just take the first one\n",
    "        print(f\"Demonstrating pipeline on sample image: {os.path.basename(sample_metadata)}\")\n",
    "        visualize_single_image_pipeline(sample_metadata, model_path)\n",
    "    else:\n",
    "        print(\"⚠️  No metadata files found for visualization demo\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping visualization demo - no model available\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
