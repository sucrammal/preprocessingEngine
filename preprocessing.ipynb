{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631451e9",
   "metadata": {},
   "source": [
    "# Burner Detection Preprocessing Engine\n",
    "\n",
    "**Pipeline Overview:** Download model → Process images → Evaluate burner classification\n",
    "\n",
    "**Key Features:**\n",
    "1. **Simple Evaluation**: Presence/absence of burners (binary classification)\n",
    "2. **Advanced Evaluation**: Spatial IoU-based matching (object detection metrics)  \n",
    "3. **Preprocessing Comparison**: Test different normalization techniques for lighting variations\n",
    "4. **Visualization Demo**: Visual walkthrough of pipeline on sample image\n",
    "\n",
    "**Preprocessing Methods Available:**\n",
    "- **Simple**: Standard 0-1 normalization (baseline)\n",
    "- **GCN**: Global Contrast Normalization (handles overall brightness differences)\n",
    "- **LCN**: Local Contrast Normalization (handles local lighting/shadow variations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e818619",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3760475-7f59-4373-90a1-3784f885f769",
   "metadata": {},
   "source": [
    "In your terminal, use the following commands to create a python3.11 kernel. Select the kernel \"Python 3.11\" at the top of the notebook to use it:\n",
    "\n",
    "```\n",
    "conda create -n my_env python=3.11 ipykernel\n",
    "conda activate my_env\n",
    "python -m ipykernel install --user --name=my_env --display-name=\"Python 3.11\"   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4add5d3-4b43-403d-9811-a13884d48f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv>=1.0.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: tensorflow-macos>=2.13.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-metal>=1.0.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (11.3.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (3.10.3)\n",
      "Requirement already satisfied: scipy>=1.9.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.16.0)\n",
      "Collecting opencv-python>=4.0.0 (from -r requirements.txt (line 9))\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-macosx_13_0_arm64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from pandas>=1.5.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from pandas>=1.5.0->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from pandas>=1.5.0->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tensorflow==2.16.2 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (3.14.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (1.73.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (3.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (0.37.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (2025.6.15)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from tensorflow-metal>=1.0.0->-r requirements.txt (line 4)) (0.45.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (3.2.3)\n",
      "INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (14.0.0)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/my_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos>=2.13.0->-r requirements.txt (line 3)) (0.1.2)\n",
      "Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e7cd929",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: pan-burner-v3 v2025-01-22T12-31-54\n",
      "Data: 8432 metadata files\n",
      "\n",
      "📋 Sample ground truth format:\n",
      "  - Label: 'burner'\n",
      "    Normalized coordinates:\n",
      "      x_min: 0.006\n",
      "      y_min: 0.320\n",
      "      x_max: 0.269\n",
      "      y_max: 0.842\n",
      "    Box dimensions (normalized):\n",
      "      width: 0.263\n",
      "      height: 0.522\n",
      "  - Label: 'burner'\n",
      "    Normalized coordinates:\n",
      "      x_min: 0.374\n",
      "      y_min: 0.459\n",
      "      x_max: 0.773\n",
      "      y_max: 0.979\n",
      "    Box dimensions (normalized):\n",
      "      width: 0.398\n",
      "      height: 0.520\n",
      "  - Label: 'burner'\n",
      "    Normalized coordinates:\n",
      "      x_min: 0.467\n",
      "      y_min: 0.021\n",
      "      x_max: 0.794\n",
      "      y_max: 0.387\n",
      "    Box dimensions (normalized):\n",
      "      width: 0.327\n",
      "      height: 0.366\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import tensorflow as tf\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "VIAM_CONFIG = {\n",
    "    \"model_name\": os.getenv(\"VIAM_MODEL_NAME\", \"your-burner-detection-model\"),\n",
    "    \"model_org_id\": os.getenv(\"VIAM_MODEL_ORG_ID\", \"your-model-org-id\"),\n",
    "    \"model_version\": os.getenv(\"VIAM_MODEL_VERSION\", \"2024-XX-XXTXX-XX-XX\"),\n",
    "}\n",
    "\n",
    "METADATA_DIR = os.getenv(\"METADATA_DIR\", \"metadata\")\n",
    "IMAGES_DIR = os.getenv(\"IMAGES_DIR\", \"data\")\n",
    "MODEL_DIR = os.getenv(\"MODEL_DIR\", \"models\")\n",
    "\n",
    "MODEL_INPUT_SIZE = None\n",
    "\n",
    "print(f\"Model: {VIAM_CONFIG['model_name']} v{VIAM_CONFIG['model_version']}\")\n",
    "print(f\"Data: {len(glob.glob(os.path.join(METADATA_DIR, '*.json')))} metadata files\")\n",
    "\n",
    "# Quick preview of ground truth format\n",
    "metadata_files_preview = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "if metadata_files_preview:\n",
    "    print(f\"\\n📋 Sample ground truth format:\")\n",
    "    with open(metadata_files_preview[0], 'r') as f:\n",
    "        sample_metadata = json.load(f)\n",
    "    if 'annotations' in sample_metadata:\n",
    "        for bbox in sample_metadata['annotations'].get('bboxes', [])[:3]:  # Show first 3\n",
    "            print(f\"  - Label: '{bbox.get('label', 'N/A')}'\")\n",
    "            print(f\"    Normalized coordinates:\")\n",
    "            print(f\"      x_min: {bbox.get('xMinNormalized', 'N/A'):.3f}\")\n",
    "            print(f\"      y_min: {bbox.get('yMinNormalized', 'N/A'):.3f}\") \n",
    "            print(f\"      x_max: {bbox.get('xMaxNormalized', 'N/A'):.3f}\")\n",
    "            print(f\"      y_max: {bbox.get('yMaxNormalized', 'N/A'):.3f}\")\n",
    "            width = bbox.get('xMaxNormalized', 0) - bbox.get('xMinNormalized', 0)\n",
    "            height = bbox.get('yMaxNormalized', 0) - bbox.get('yMinNormalized', 0)\n",
    "            print(f\"    Box dimensions (normalized):\")\n",
    "            print(f\"      width: {width:.3f}\")\n",
    "            print(f\"      height: {height:.3f}\")\n",
    "    else:\n",
    "        print(\"  No annotations found in sample metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81590da-2f1d-4ff0-9c85-aca3bed4074b",
   "metadata": {},
   "source": [
    "### Preprocessing and data checks: \n",
    "- Select between simple normalization (simple), local contrast normalization (LCN), and global contrast normalization (GCN).\n",
    "- Check if all bounding box keys are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356ce01-a889-40ef-8246-20478f6e0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_global_contrast_normalization(image_array: np.ndarray, epsilon: float = 1e-8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply Global Contrast Normalization (GCN)\n",
    "    \n",
    "    This enhances global contrast by standardizing the entire image to have\n",
    "    zero mean and unit variance, then scaling to 0-1 range.\n",
    "    \"\"\"\n",
    "    # Convert to float for calculations\n",
    "    image_float = image_array.astype(np.float32)\n",
    "    \n",
    "    # Global mean and std across all pixels and channels\n",
    "    global_mean = np.mean(image_float)\n",
    "    global_std = np.std(image_float)\n",
    "    \n",
    "    # Standardize: zero mean, unit variance\n",
    "    normalized = (image_float - global_mean) / (global_std + epsilon)\n",
    "    \n",
    "    # Scale to 0-1 range for model input\n",
    "    # Use a more aggressive scaling to show the effect\n",
    "    normalized = np.tanh(normalized * 0.5) * 0.5 + 0.5  # Tanh activation for contrast\n",
    "    \n",
    "    return np.clip(normalized, 0, 1)\n",
    "\n",
    "def apply_local_contrast_normalization(image_array: np.ndarray, window_size: int = 9, epsilon: float = 1e-8) -> np.ndarray:\n",
    "    \"\"\"Apply Local Contrast Normalization (LCN)\"\"\"\n",
    "    normalized = np.zeros_like(image_array, dtype=np.float32)\n",
    "    \n",
    "    for channel in range(image_array.shape[2]):\n",
    "        channel_data = image_array[:, :, channel]\n",
    "        local_mean = ndimage.uniform_filter(channel_data, size=window_size, mode='reflect')\n",
    "        local_variance = ndimage.uniform_filter(channel_data**2, size=window_size, mode='reflect') - local_mean**2\n",
    "        local_std = np.sqrt(np.maximum(local_variance, 0)) + epsilon\n",
    "        channel_normalized = (channel_data - local_mean) / local_std\n",
    "        channel_normalized = (channel_normalized - channel_normalized.min()) / (channel_normalized.max() - channel_normalized.min() + epsilon)\n",
    "        normalized[:, :, channel] = channel_normalized\n",
    "    \n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645014a2-a1b2-44ef-91bd-df69dead7e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_array: np.ndarray, target_size: Optional[Tuple[int, int]] = None,\n",
    "                    normalization_method: str = \"simple\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess image array for model input\n",
    "    \n",
    "    Args:\n",
    "        image_array: Input image as numpy array\n",
    "        target_size: Target size for model input (None to keep original size)\n",
    "        normalization_method: 'simple', 'gcn', or 'lcn'\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image array ready for model input\n",
    "    \"\"\"\n",
    "    # Resize image if target_size is specified\n",
    "    if target_size is not None:\n",
    "        image_pil = Image.fromarray(image_array)\n",
    "        image_resized = image_pil.resize(target_size)\n",
    "        image_array = np.array(image_resized, dtype=np.float32)\n",
    "    else:\n",
    "        # Keep original size\n",
    "        image_array = np.array(image_array, dtype=np.float32)\n",
    "    \n",
    "    # Apply normalization\n",
    "    if normalization_method == \"simple\":\n",
    "        # Simple 0-1 normalization\n",
    "        normalized = image_array / 255.0\n",
    "    elif normalization_method == \"gcn\":\n",
    "        # Global Contrast Normalization\n",
    "        normalized = apply_global_contrast_normalization(image_array)\n",
    "    elif normalization_method == \"lcn\":\n",
    "        # Local Contrast Normalization\n",
    "        normalized = apply_local_contrast_normalization(image_array)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization method: {normalization_method}\")\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def visualize_preprocessing_effects(image_array: np.ndarray, methods: List[str] = [\"simple\", \"gcn\", \"lcn\"], \n",
    "                                   target_size: Optional[Tuple[int, int]] = None):\n",
    "    \"\"\"Visualize the effects of different preprocessing methods with detailed analysis\"\"\"\n",
    "    \n",
    "    # Create subplots: 2 rows (images + histograms), multiple columns\n",
    "    fig, axes = plt.subplots(2, len(methods) + 1, figsize=(5 * (len(methods) + 1), 10))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(image_array.astype(np.uint8))\n",
    "    axes[0, 0].set_title(\"Original Image\")\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Original histogram\n",
    "    axes[1, 0].hist(image_array.flatten(), bins=50, alpha=0.7, color='blue', range=(0, 255))\n",
    "    axes[1, 0].set_title(\"Original Histogram\")\n",
    "    axes[1, 0].set_xlabel(\"Pixel Value\")\n",
    "    axes[1, 0].set_ylabel(\"Frequency\")\n",
    "    axes[1, 0].set_xlim(0, 255)\n",
    "    \n",
    "    # Store statistics for comparison\n",
    "    stats = []\n",
    "    stats.append({\n",
    "        'method': 'Original',\n",
    "        'mean': np.mean(image_array),\n",
    "        'std': np.std(image_array),\n",
    "        'min': np.min(image_array),\n",
    "        'max': np.max(image_array)\n",
    "    })\n",
    "    \n",
    "    # Preprocessed versions\n",
    "    for i, method in enumerate(methods):\n",
    "        preprocessed = preprocess_image(image_array, target_size, method)\n",
    "        \n",
    "        # For display, we need to handle the different ranges\n",
    "        if method == \"simple\":\n",
    "            # Simple is already 0-1, just scale to display\n",
    "            display_image = (preprocessed * 255).astype(np.uint8)\n",
    "            hist_data = preprocessed.flatten() * 255  # Scale for histogram\n",
    "            hist_range = (0, 255)\n",
    "        else:\n",
    "            # For GCN and LCN, show the actual normalized values but scale for display\n",
    "            # Clip to reasonable range for display\n",
    "            display_preprocessed = np.clip(preprocessed, 0, 1)\n",
    "            display_image = (display_preprocessed * 255).astype(np.uint8)\n",
    "            hist_data = preprocessed.flatten()\n",
    "            hist_range = (np.min(hist_data), np.max(hist_data))\n",
    "        \n",
    "        # Display preprocessed image\n",
    "        axes[0, i + 1].imshow(display_image)\n",
    "        axes[0, i + 1].set_title(f\"{method.upper()} Preprocessing\")\n",
    "        axes[0, i + 1].axis('off')\n",
    "        \n",
    "        # Display histogram\n",
    "        axes[1, i + 1].hist(hist_data, bins=50, alpha=0.7, \n",
    "                           color=['green', 'orange', 'red'][i], range=hist_range)\n",
    "        axes[1, i + 1].set_title(f\"{method.upper()} Histogram\")\n",
    "        axes[1, i + 1].set_xlabel(\"Normalized Value\")\n",
    "        axes[1, i + 1].set_ylabel(\"Frequency\")\n",
    "        \n",
    "        # Store statistics\n",
    "        stats.append({\n",
    "            'method': method.upper(),\n",
    "            'mean': np.mean(preprocessed),\n",
    "            'std': np.std(preprocessed),\n",
    "            'min': np.min(preprocessed),\n",
    "            'max': np.max(preprocessed)\n",
    "        })\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"\\n📊 PREPROCESSING STATISTICS COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Method':<10} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    for stat in stats:\n",
    "        print(f\"{stat['method']:<10} {stat['mean']:<10.3f} {stat['std']:<10.3f} \"\n",
    "              f\"{stat['min']:<10.3f} {stat['max']:<10.3f}\")\n",
    "    \n",
    "    print(\"\\n💡 What to look for:\")\n",
    "    print(\"   - SIMPLE: Should have mean ≈ original/255, range [0,1]\")\n",
    "    print(\"   - GCN: Should have different mean/std, enhanced global contrast\")  \n",
    "    print(\"   - LCN: Should show enhanced local features, may have wider range\")\n",
    "    print(\"   - Histograms show the distribution of pixel values after normalization\")\n",
    "\n",
    "def visualize_preprocessing_detail_comparison(image_array: np.ndarray, \n",
    "                                            methods: List[str] = [\"simple\", \"gcn\", \"lcn\"],\n",
    "                                            crop_size: int = 150):\n",
    "    \"\"\"\n",
    "    Show detailed side-by-side comparison of preprocessing effects on a cropped region\n",
    "    This makes the differences more visible by focusing on a smaller area\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔍 DETAILED PREPROCESSING COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Select a center crop to focus on details\n",
    "    h, w = image_array.shape[:2]\n",
    "    center_y, center_x = h // 2, w // 2\n",
    "    y1 = max(0, center_y - crop_size // 2)\n",
    "    y2 = min(h, center_y + crop_size // 2)\n",
    "    x1 = max(0, center_x - crop_size // 2)\n",
    "    x2 = min(w, center_x + crop_size // 2)\n",
    "    \n",
    "    cropped_original = image_array[y1:y2, x1:x2]\n",
    "    \n",
    "    # Create comparison figure\n",
    "    fig, axes = plt.subplots(2, len(methods) + 1, figsize=(4 * (len(methods) + 1), 8))\n",
    "    \n",
    "    # Original cropped\n",
    "    axes[0, 0].imshow(cropped_original.astype(np.uint8))\n",
    "    axes[0, 0].set_title(\"Original (Cropped)\")\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Show a line profile across the middle\n",
    "    middle_line = cropped_original[cropped_original.shape[0]//2, :, 0]  # Red channel\n",
    "    axes[1, 0].plot(middle_line, 'b-', linewidth=2, label='Original')\n",
    "    axes[1, 0].set_title(\"Pixel Intensity Profile\")\n",
    "    axes[1, 0].set_xlabel(\"Pixel Position\")\n",
    "    axes[1, 0].set_ylabel(\"Intensity\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Process each method\n",
    "    colors = ['green', 'orange', 'red']\n",
    "    for i, method in enumerate(methods):\n",
    "        # Apply preprocessing to full image then crop\n",
    "        preprocessed_full = preprocess_image(image_array, None, method)\n",
    "        preprocessed_crop = preprocessed_full[y1:y2, x1:x2]\n",
    "        \n",
    "        # Display cropped preprocessed image\n",
    "        display_image = (preprocessed_crop * 255).astype(np.uint8)\n",
    "        axes[0, i + 1].imshow(display_image)\n",
    "        axes[0, i + 1].set_title(f\"{method.upper()} (Cropped)\")\n",
    "        axes[0, i + 1].axis('off')\n",
    "        \n",
    "        # Show line profile for comparison\n",
    "        processed_line = preprocessed_crop[preprocessed_crop.shape[0]//2, :, 0] * 255  # Scale for comparison\n",
    "        axes[1, i + 1].plot(middle_line, 'b-', linewidth=2, alpha=0.7, label='Original')\n",
    "        axes[1, i + 1].plot(processed_line, color=colors[i], linewidth=2, label=f'{method.upper()}')\n",
    "        axes[1, i + 1].set_title(f\"{method.upper()} vs Original\")\n",
    "        axes[1, i + 1].set_xlabel(\"Pixel Position\")\n",
    "        axes[1, i + 1].set_ylabel(\"Intensity\")\n",
    "        axes[1, i + 1].legend()\n",
    "        axes[1, i + 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print observations\n",
    "    print(f\"\\n📋 Observations from cropped region ({crop_size}x{crop_size} pixels):\")\n",
    "    print(\"   Line profiles show pixel intensity changes across the middle row\")\n",
    "    print(\"   - SIMPLE: Should closely follow original (just scaled)\")\n",
    "    print(\"   - GCN: May show enhanced contrast globally\") \n",
    "    print(\"   - LCN: Should show enhanced local details and edge contrast\")\n",
    "\n",
    "print(\"✅ Preprocessing functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221fd379",
   "metadata": {},
   "source": [
    "## Data validation and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "111726f6-0fb1-4626-86ee-72e1848546f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_for_metadata(metadata_file: str) -> str:\n",
    "    \"\"\"Find corresponding image file for metadata\"\"\"\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    filename = metadata.get('fileName', '')\n",
    "    binary_id = metadata.get('id', '')\n",
    "    \n",
    "    # Try direct filename match\n",
    "    image_path = os.path.join(IMAGES_DIR, filename)\n",
    "    if os.path.exists(image_path):\n",
    "        return image_path\n",
    "    \n",
    "    # Try binary ID match\n",
    "    for file in os.listdir(IMAGES_DIR):\n",
    "        if binary_id in file and file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            return os.path.join(IMAGES_DIR, file)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_burner_bounding_boxes(metadata: Dict) -> List[Tuple[float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Extract burner bounding boxes from metadata\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (xmin, ymin, xmax, ymax) in normalized coordinates\n",
    "    \"\"\"\n",
    "    burner_boxes = []\n",
    "    \n",
    "    if 'annotations' in metadata:\n",
    "        for bbox in metadata['annotations'].get('bboxes', []):\n",
    "            label = bbox.get('label', '')\n",
    "            if 'burner' in label.lower():\n",
    "                # Check if all coordinates are present\n",
    "                required_keys = [\"xMinNormalized\", \"yMinNormalized\", \"xMaxNormalized\", \"yMaxNormalized\"]\n",
    "                if all(key in bbox for key in required_keys):\n",
    "                    burner_box = (\n",
    "                        bbox[\"xMinNormalized\"],\n",
    "                        bbox[\"yMinNormalized\"],\n",
    "                        bbox[\"xMaxNormalized\"],\n",
    "                        bbox[\"yMaxNormalized\"]\n",
    "                    )\n",
    "                    burner_boxes.append(burner_box)\n",
    "    \n",
    "    return burner_boxes\n",
    "\n",
    "def is_valid_bbox(bbox: Dict) -> bool:\n",
    "    \"\"\"Check if a bounding box has all required coordinate keys\"\"\"\n",
    "    required_keys = [\"xMinNormalized\", \"yMinNormalized\", \"xMaxNormalized\", \"yMaxNormalized\"]\n",
    "    return all(key in bbox for key in required_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0255c00-f3ad-4367-805c-9531e1b5d2d2",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a1cd2a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_dataset_dataframe(max_images: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create dataset DataFrame with images and ground truth\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: image_name, image_path, image, burner_bounding_boxes\n",
    "    \"\"\"\n",
    "    print(\"\\n🗂️ CREATING DATASET DATAFRAME\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get all metadata files\n",
    "    metadata_files = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "    \n",
    "    if max_images:\n",
    "        metadata_files = metadata_files[:max_images]\n",
    "        print(f\"Processing first {max_images} images for testing\")\n",
    "    \n",
    "    if not metadata_files:\n",
    "        print(\"❌ No metadata files found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Initialize lists\n",
    "    image_names = []\n",
    "    image_paths = []\n",
    "    images = []\n",
    "    burner_bboxes = []\n",
    "    \n",
    "    print(f\"📋 Processing {len(metadata_files)} metadata files...\")\n",
    "    \n",
    "    for i, metadata_file in enumerate(metadata_files):\n",
    "        try:\n",
    "            # Load metadata\n",
    "            with open(metadata_file, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            # Find corresponding image\n",
    "            image_path = find_image_for_metadata(metadata_file)\n",
    "            if not image_path:\n",
    "                print(f\"⚠️  No image found for {os.path.basename(metadata_file)}\")\n",
    "                continue\n",
    "            \n",
    "            # Load image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_array = np.array(image)\n",
    "            \n",
    "            # Extract burner bounding boxes\n",
    "            bboxes = extract_burner_bounding_boxes(metadata)\n",
    "            \n",
    "            # Add to lists\n",
    "            image_names.append(os.path.basename(image_path))\n",
    "            image_paths.append(image_path)\n",
    "            images.append(image_array)\n",
    "            burner_bboxes.append(bboxes)\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"   Progress: {i + 1}/{len(metadata_files)} images processed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {os.path.basename(metadata_file)}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'image_name': image_names,\n",
    "        'image_path': image_paths,\n",
    "        'image': images,\n",
    "        'burner_bounding_boxes': burner_bboxes\n",
    "    })\n",
    "    \n",
    "    # Add derived columns\n",
    "    df['num_burners'] = df['burner_bounding_boxes'].apply(len)\n",
    "    df['has_burners'] = df['num_burners'] > 0\n",
    "    \n",
    "    print(f\"\\n✅ Dataset DataFrame created!\")\n",
    "    print(f\"   Total images: {len(df)}\")\n",
    "    print(f\"   Images with burners: {len(df[df['has_burners']])}\")\n",
    "    print(f\"   Images without burners: {len(df[~df['has_burners']])}\")\n",
    "    print(f\"   Total burner objects: {df['num_burners'].sum()}\")\n",
    "    print(f\"   Average burners per image: {df['num_burners'].mean():.2f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def visualize_dataset_samples(df: pd.DataFrame, num_samples: int = 6):\n",
    "    \"\"\"Visualize sample images from the dataset\"\"\"\n",
    "    print(f\"\\n📸 VISUALIZING {num_samples} DATASET SAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Select diverse samples\n",
    "    with_burners = df[df['has_burners']].head(num_samples // 2)\n",
    "    without_burners = df[~df['has_burners']].head(num_samples // 2)\n",
    "    samples = pd.concat([with_burners, without_burners])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples // 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows()):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Display image\n",
    "        axes[i].imshow(row['image'])\n",
    "        axes[i].set_title(f\"{row['image_name']}\\n{row['num_burners']} burners\")\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for bbox in row['burner_bounding_boxes']:\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            h, w = row['image'].shape[:2]\n",
    "            \n",
    "            # Convert to pixel coordinates\n",
    "            x1, y1 = int(xmin * w), int(ymin * h)\n",
    "            x2, y2 = int(xmax * w), int(ymax * h)\n",
    "            \n",
    "            # Draw rectangle\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, \n",
    "                                   linewidth=2, edgecolor='red', facecolor='none')\n",
    "            axes[i].add_patch(rect)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "print(\"\\n🚀 STEP 2: DATASET CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For testing, limit to first 100 images - remove this for full dataset\n",
    "# dataset_df = create_dataset_dataframe(max_images=100)\n",
    "dataset_df = create_dataset_dataframe()\n",
    "\n",
    "if not dataset_df.empty:\n",
    "    # Visualize samples\n",
    "    visualize_dataset_samples(dataset_df)\n",
    "    \n",
    "    # Show preprocessing effects on a sample\n",
    "    if len(dataset_df) > 0:\n",
    "        sample_image = dataset_df.iloc[0]['image']\n",
    "        print(f\"\\n🔍 PREPROCESSING EFFECTS ON SAMPLE IMAGE\")\n",
    "        print(f\"Original image size: {sample_image.shape[:2]} (height x width)\")\n",
    "        visualize_preprocessing_effects(sample_image)\n",
    "        visualize_preprocessing_detail_comparison(sample_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf146a65",
   "metadata": {},
   "source": [
    "## Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0c9d2d4",
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 DOWNLOADING MODEL\n",
      "============================================================\n",
      "📥 Downloading model...\n",
      "✅ Model downloaded successfully\n",
      "✅ Model found: models/2025-05-14T11-12-00/pan-burner-v4.tflite\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models/2025-05-14T11-12-00/pan-burner-v4.tflite'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_model():\n",
    "    \"\"\"Download TFLite model from Viam\"\"\"\n",
    "    import tarfile\n",
    "    \n",
    "    print(\"\\n🤖 DOWNLOADING MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    \n",
    "    cmd = [\n",
    "        \"viam\", \"packages\", \"export\",\n",
    "        \"--org-id\", VIAM_CONFIG[\"model_org_id\"],\n",
    "        \"--name\", VIAM_CONFIG[\"model_name\"],\n",
    "        \"--version\", VIAM_CONFIG[\"model_version\"],\n",
    "        \"--type\", \"ml_model\",\n",
    "        \"--destination\", MODEL_DIR\n",
    "    ]\n",
    "    \n",
    "    print(\"📥 Downloading model...\")\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ Model downloaded successfully\")\n",
    "        \n",
    "        # Look for existing .tflite files\n",
    "        tflite_files = glob.glob(os.path.join(MODEL_DIR, \"**/*.tflite\"), recursive=True)\n",
    "        if tflite_files:\n",
    "            model_path = tflite_files[0]\n",
    "            print(f\"✅ Model found: {model_path}\")\n",
    "            return model_path\n",
    "        \n",
    "        # Extract .tar.gz files if needed\n",
    "        tar_files = glob.glob(os.path.join(MODEL_DIR, \"**/*.tar.gz\"), recursive=True)\n",
    "        if tar_files:\n",
    "            for tar_file in tar_files:\n",
    "                print(f\"📦 Extracting {tar_file}...\")\n",
    "                try:\n",
    "                    with tarfile.open(tar_file, 'r:gz') as tar:\n",
    "                        tar.extractall(os.path.dirname(tar_file))\n",
    "                    print(f\"✅ Extracted {tar_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error extracting {tar_file}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Look for .tflite files after extraction\n",
    "            tflite_files = glob.glob(os.path.join(MODEL_DIR, \"**/*.tflite\"), recursive=True)\n",
    "            if tflite_files:\n",
    "                model_path = tflite_files[0]\n",
    "                print(f\"✅ Model ready: {model_path}\")\n",
    "                return model_path\n",
    "        \n",
    "        print(\"❌ No .tflite file found\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"❌ Download failed: {result.stderr}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6973da-f4b0-4193-a8f4-0b48636ed8df",
   "metadata": {},
   "source": [
    "### Load in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ebf90d3b-2606-4ab6-87d6-cd43d8fb63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path: str):\n",
    "    \"\"\"Load TFLite model and return interpreter\"\"\"\n",
    "    print(f\"\\n🔄 LOADING MODEL: {model_path}\")\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input/output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    print(\"✅ Model loaded successfully\")\n",
    "    print(f\"   Input shape: {input_details[0]['shape']}\")\n",
    "    print(f\"   Input dtype: {input_details[0]['dtype']}\")\n",
    "    print(f\"   Output tensors: {len(output_details)}\")\n",
    "    \n",
    "    # Show what input size will be used\n",
    "    input_shape = input_details[0]['shape']\n",
    "    if len(input_shape) == 4:  # [batch, height, width, channels]\n",
    "        if input_shape[1] > 1 and input_shape[2] > 1:\n",
    "            print(f\"   Model expects images resized to: {input_shape[2]}x{input_shape[1]} (width x height)\")\n",
    "        else:\n",
    "            print(f\"   Model accepts variable input sizes (no resizing needed)\")\n",
    "            print(f\"   Input shape: {input_shape} (where -1 means dynamic)\")\n",
    "    else:\n",
    "        print(f\"   Model input shape is not standard image format: {input_shape}\")\n",
    "    \n",
    "    return interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d4e68cfa-6577-4c93-9163-de060c3927a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input_size(interpreter) -> Optional[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Get the expected input size from the model interpreter\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (width, height) if model has fixed input size, None if variable\n",
    "    \"\"\"\n",
    "    input_details = interpreter.get_input_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    \n",
    "    if len(input_shape) == 4:  # [batch, height, width, channels]\n",
    "        if input_shape[1] > 1 and input_shape[2] > 1:\n",
    "            return (input_shape[2], input_shape[1])  # (width, height)\n",
    "        else:\n",
    "            return None  # Variable input size\n",
    "    else:\n",
    "        return None  # Non-standard input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c458afec-a2c8-4bd9-9b51-229b2bbeb7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 MODEL LOADING\n",
      "============================================================\n",
      "\n",
      "🤖 DOWNLOADING MODEL\n",
      "============================================================\n",
      "📥 Downloading model...\n",
      "✅ Model downloaded successfully\n",
      "✅ Model found: models/2025-05-14T11-12-00/pan-burner-v4.tflite\n",
      "\n",
      "🔄 LOADING MODEL: models/2025-05-14T11-12-00/pan-burner-v4.tflite\n",
      "✅ Model loaded successfully\n",
      "   Input shape: [  1 384 384   3]\n",
      "   Input dtype: <class 'numpy.uint8'>\n",
      "   Output tensors: 4\n",
      "   Model expects images resized to: 384x384 (width x height)\n",
      "🔧 Images will be resized to 384x384 for inference\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🚀 MODEL LOADING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_path = download_model()\n",
    "if model_path:\n",
    "    model_interpreter = load_model(model_path)\n",
    "    \n",
    "    # Show what preprocessing will be used\n",
    "    expected_size = get_model_input_size(model_interpreter)\n",
    "    if expected_size:\n",
    "        print(f\"🔧 Images will be resized to {expected_size[0]}x{expected_size[1]} for inference\")\n",
    "    else:\n",
    "        print(f\"🔧 Images will keep their original size for inference\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No model available - skipping inference steps\")\n",
    "    model_interpreter = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb104f-7ef3-4f91-b06a-e80badded84c",
   "metadata": {},
   "source": [
    "### Infernece: Call the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b78a6d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 STEP 4: INFERENCE\n",
      "============================================================\n",
      "\n",
      "🎯 RUNNING INFERENCE ON 8126 IMAGES\n",
      "============================================================\n",
      "Using simple normalization\n",
      "   Progress: 25/8126 images processed\n",
      "   Progress: 50/8126 images processed\n",
      "   Progress: 75/8126 images processed\n",
      "   Progress: 100/8126 images processed\n",
      "   Progress: 125/8126 images processed\n",
      "   Progress: 150/8126 images processed\n",
      "   Progress: 175/8126 images processed\n",
      "   Progress: 200/8126 images processed\n",
      "   Progress: 225/8126 images processed\n",
      "   Progress: 250/8126 images processed\n",
      "   Progress: 275/8126 images processed\n",
      "   Progress: 300/8126 images processed\n",
      "   Progress: 325/8126 images processed\n",
      "   Progress: 350/8126 images processed\n",
      "   Progress: 375/8126 images processed\n",
      "   Progress: 400/8126 images processed\n",
      "   Progress: 425/8126 images processed\n",
      "   Progress: 450/8126 images processed\n",
      "   Progress: 475/8126 images processed\n",
      "   Progress: 500/8126 images processed\n",
      "   Progress: 525/8126 images processed\n",
      "   Progress: 550/8126 images processed\n",
      "   Progress: 575/8126 images processed\n",
      "   Progress: 600/8126 images processed\n",
      "   Progress: 625/8126 images processed\n",
      "   Progress: 650/8126 images processed\n",
      "   Progress: 675/8126 images processed\n",
      "   Progress: 700/8126 images processed\n",
      "   Progress: 725/8126 images processed\n",
      "   Progress: 750/8126 images processed\n",
      "   Progress: 775/8126 images processed\n",
      "   Progress: 800/8126 images processed\n",
      "   Progress: 825/8126 images processed\n",
      "   Progress: 850/8126 images processed\n",
      "   Progress: 875/8126 images processed\n",
      "   Progress: 900/8126 images processed\n",
      "   Progress: 925/8126 images processed\n",
      "   Progress: 950/8126 images processed\n",
      "   Progress: 975/8126 images processed\n",
      "   Progress: 1000/8126 images processed\n",
      "   Progress: 1025/8126 images processed\n",
      "   Progress: 1050/8126 images processed\n",
      "   Progress: 1075/8126 images processed\n",
      "   Progress: 1100/8126 images processed\n",
      "   Progress: 1125/8126 images processed\n",
      "   Progress: 1150/8126 images processed\n",
      "   Progress: 1175/8126 images processed\n",
      "   Progress: 1200/8126 images processed\n",
      "   Progress: 1225/8126 images processed\n",
      "   Progress: 1250/8126 images processed\n",
      "   Progress: 1275/8126 images processed\n",
      "   Progress: 1300/8126 images processed\n",
      "   Progress: 1325/8126 images processed\n",
      "   Progress: 1350/8126 images processed\n",
      "   Progress: 1375/8126 images processed\n",
      "   Progress: 1400/8126 images processed\n",
      "   Progress: 1425/8126 images processed\n",
      "   Progress: 1450/8126 images processed\n",
      "   Progress: 1475/8126 images processed\n",
      "   Progress: 1500/8126 images processed\n",
      "   Progress: 1525/8126 images processed\n",
      "   Progress: 1550/8126 images processed\n",
      "   Progress: 1575/8126 images processed\n",
      "   Progress: 1600/8126 images processed\n",
      "   Progress: 1625/8126 images processed\n",
      "   Progress: 1650/8126 images processed\n",
      "   Progress: 1675/8126 images processed\n",
      "   Progress: 1700/8126 images processed\n",
      "   Progress: 1725/8126 images processed\n",
      "   Progress: 1750/8126 images processed\n",
      "   Progress: 1775/8126 images processed\n",
      "   Progress: 1800/8126 images processed\n",
      "   Progress: 1825/8126 images processed\n",
      "   Progress: 1850/8126 images processed\n",
      "   Progress: 1875/8126 images processed\n",
      "   Progress: 1900/8126 images processed\n",
      "   Progress: 1925/8126 images processed\n",
      "   Progress: 1950/8126 images processed\n",
      "   Progress: 1975/8126 images processed\n",
      "   Progress: 2000/8126 images processed\n",
      "   Progress: 2025/8126 images processed\n",
      "   Progress: 2050/8126 images processed\n",
      "   Progress: 2075/8126 images processed\n",
      "   Progress: 2100/8126 images processed\n",
      "   Progress: 2125/8126 images processed\n",
      "   Progress: 2150/8126 images processed\n",
      "   Progress: 2175/8126 images processed\n",
      "   Progress: 2200/8126 images processed\n",
      "   Progress: 2225/8126 images processed\n",
      "   Progress: 2250/8126 images processed\n",
      "   Progress: 2275/8126 images processed\n",
      "   Progress: 2300/8126 images processed\n",
      "   Progress: 2325/8126 images processed\n",
      "   Progress: 2350/8126 images processed\n",
      "   Progress: 2375/8126 images processed\n",
      "   Progress: 2400/8126 images processed\n",
      "   Progress: 2425/8126 images processed\n",
      "   Progress: 2450/8126 images processed\n",
      "   Progress: 2475/8126 images processed\n",
      "   Progress: 2500/8126 images processed\n",
      "   Progress: 2525/8126 images processed\n",
      "   Progress: 2550/8126 images processed\n",
      "   Progress: 2575/8126 images processed\n",
      "   Progress: 2600/8126 images processed\n",
      "   Progress: 2625/8126 images processed\n",
      "   Progress: 2650/8126 images processed\n",
      "   Progress: 2675/8126 images processed\n",
      "   Progress: 2700/8126 images processed\n",
      "   Progress: 2725/8126 images processed\n",
      "   Progress: 2750/8126 images processed\n",
      "   Progress: 2775/8126 images processed\n",
      "   Progress: 2800/8126 images processed\n",
      "   Progress: 2825/8126 images processed\n",
      "   Progress: 2850/8126 images processed\n",
      "   Progress: 2875/8126 images processed\n",
      "   Progress: 2900/8126 images processed\n",
      "   Progress: 2925/8126 images processed\n",
      "   Progress: 2950/8126 images processed\n",
      "   Progress: 2975/8126 images processed\n",
      "   Progress: 3000/8126 images processed\n",
      "   Progress: 3025/8126 images processed\n",
      "   Progress: 3050/8126 images processed\n",
      "   Progress: 3075/8126 images processed\n",
      "   Progress: 3100/8126 images processed\n",
      "   Progress: 3125/8126 images processed\n",
      "   Progress: 3150/8126 images processed\n",
      "   Progress: 3175/8126 images processed\n",
      "   Progress: 3200/8126 images processed\n",
      "   Progress: 3225/8126 images processed\n",
      "   Progress: 3250/8126 images processed\n",
      "   Progress: 3275/8126 images processed\n",
      "   Progress: 3300/8126 images processed\n",
      "   Progress: 3325/8126 images processed\n",
      "   Progress: 3350/8126 images processed\n",
      "   Progress: 3375/8126 images processed\n",
      "   Progress: 3400/8126 images processed\n",
      "   Progress: 3425/8126 images processed\n",
      "   Progress: 3450/8126 images processed\n",
      "   Progress: 3475/8126 images processed\n",
      "   Progress: 3500/8126 images processed\n",
      "   Progress: 3525/8126 images processed\n",
      "   Progress: 3550/8126 images processed\n",
      "   Progress: 3575/8126 images processed\n",
      "   Progress: 3600/8126 images processed\n",
      "   Progress: 3625/8126 images processed\n",
      "   Progress: 3650/8126 images processed\n",
      "   Progress: 3675/8126 images processed\n",
      "   Progress: 3700/8126 images processed\n",
      "   Progress: 3725/8126 images processed\n",
      "   Progress: 3750/8126 images processed\n",
      "   Progress: 3775/8126 images processed\n",
      "   Progress: 3800/8126 images processed\n",
      "   Progress: 3825/8126 images processed\n",
      "   Progress: 3850/8126 images processed\n",
      "   Progress: 3875/8126 images processed\n",
      "   Progress: 3900/8126 images processed\n",
      "   Progress: 3925/8126 images processed\n",
      "   Progress: 3950/8126 images processed\n",
      "   Progress: 3975/8126 images processed\n",
      "   Progress: 4000/8126 images processed\n",
      "   Progress: 4025/8126 images processed\n",
      "   Progress: 4050/8126 images processed\n",
      "   Progress: 4075/8126 images processed\n",
      "   Progress: 4100/8126 images processed\n",
      "   Progress: 4125/8126 images processed\n",
      "   Progress: 4150/8126 images processed\n",
      "   Progress: 4175/8126 images processed\n",
      "   Progress: 4200/8126 images processed\n",
      "   Progress: 4225/8126 images processed\n",
      "   Progress: 4250/8126 images processed\n",
      "   Progress: 4275/8126 images processed\n",
      "   Progress: 4300/8126 images processed\n",
      "   Progress: 4325/8126 images processed\n",
      "   Progress: 4350/8126 images processed\n",
      "   Progress: 4375/8126 images processed\n",
      "   Progress: 4400/8126 images processed\n",
      "   Progress: 4425/8126 images processed\n",
      "   Progress: 4450/8126 images processed\n",
      "   Progress: 4475/8126 images processed\n",
      "   Progress: 4500/8126 images processed\n",
      "   Progress: 4525/8126 images processed\n",
      "   Progress: 4550/8126 images processed\n",
      "   Progress: 4575/8126 images processed\n",
      "   Progress: 4600/8126 images processed\n",
      "   Progress: 4625/8126 images processed\n",
      "   Progress: 4650/8126 images processed\n",
      "   Progress: 4675/8126 images processed\n",
      "   Progress: 4700/8126 images processed\n",
      "   Progress: 4725/8126 images processed\n",
      "   Progress: 4750/8126 images processed\n"
     ]
    }
   ],
   "source": [
    "def run_single_inference(image_array: np.ndarray, interpreter, \n",
    "                        normalization_method: str = \"simple\") -> List[Tuple[float, float, float, float, float]]:\n",
    "    \"\"\"\n",
    "    Run inference on a single image\n",
    "    \n",
    "    The function automatically determines the required input size from the model:\n",
    "    - If model has fixed input size (e.g., 640x640), image will be resized\n",
    "    - If model accepts variable sizes (shape contains -1), original size is kept\n",
    "    \n",
    "    Args:\n",
    "        image_array: Input image as numpy array\n",
    "        interpreter: TFLite interpreter\n",
    "        normalization_method: Preprocessing method ('simple', 'gcn', 'lcn')\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (xmin, ymin, xmax, ymax, confidence) for detected burners\n",
    "    \"\"\"\n",
    "    # Get model details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Get expected input size from model (or use None for no resizing)\n",
    "    target_size = get_model_input_size(interpreter)\n",
    "    \n",
    "    # Preprocess image\n",
    "    preprocessed = preprocess_image(image_array, target_size, normalization_method)\n",
    "    \n",
    "    # Convert to model's expected dtype\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "    if input_dtype == np.uint8:\n",
    "        preprocessed = np.clip(preprocessed * 255.0, 0, 255).astype(np.uint8)\n",
    "    else:\n",
    "        preprocessed = preprocessed.astype(np.float32)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    input_data = np.expand_dims(preprocessed, axis=0)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get outputs\n",
    "    outputs = {}\n",
    "    for detail in output_details:\n",
    "        outputs[detail['name']] = interpreter.get_tensor(detail['index'])\n",
    "    \n",
    "    # Parse detections (adjust tensor names based on your model)\n",
    "    detections = []\n",
    "    boxes = outputs.get('detection_boxes', outputs.get('boxes', None))\n",
    "    classes = outputs.get('detection_classes', outputs.get('classes', None))\n",
    "    scores = outputs.get('detection_scores', outputs.get('scores', None))\n",
    "    \n",
    "    if boxes is not None and classes is not None and scores is not None:\n",
    "        # Remove batch dimension\n",
    "        if boxes.ndim == 3: boxes = boxes[0]\n",
    "        if classes.ndim == 2: classes = classes[0]\n",
    "        if scores.ndim == 2: scores = scores[0]\n",
    "        \n",
    "        for i, score in enumerate(scores):\n",
    "            if score > 0.5 and int(classes[i]) == 0:  # confidence threshold and burner class\n",
    "                # boxes format: [ymin, xmin, ymax, xmax]\n",
    "                ymin, xmin, ymax, xmax = boxes[i]\n",
    "                detection = (float(xmin), float(ymin), float(xmax), float(ymax), float(score))\n",
    "                detections.append(detection)\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def run_inference_on_dataframe(df: pd.DataFrame, interpreter, \n",
    "                              normalization_method: str = \"simple\") -> pd.DataFrame:\n",
    "    \"\"\"Run inference on all images in the DataFrame\"\"\"\n",
    "    \n",
    "    print(f\"\\n🎯 RUNNING INFERENCE ON {len(df)} IMAGES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Using {normalization_method} normalization\")\n",
    "    \n",
    "    inferred_bboxes = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        try:\n",
    "            detections = run_single_inference(row['image'], interpreter, normalization_method)\n",
    "            inferred_bboxes.append(detections)\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 25 == 0:\n",
    "                print(f\"   Progress: {i + 1}/{len(df)} images processed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {row['image_name']}: {e}\")\n",
    "            inferred_bboxes.append([])\n",
    "    \n",
    "    # Add results to DataFrame\n",
    "    df_with_inference = df.copy()\n",
    "    df_with_inference['inferred_burner_bboxes'] = inferred_bboxes\n",
    "    df_with_inference['num_inferred_burners'] = df_with_inference['inferred_burner_bboxes'].apply(len)\n",
    "    df_with_inference['has_inferred_burners'] = df_with_inference['num_inferred_burners'] > 0\n",
    "    \n",
    "    print(f\"\\n✅ Inference completed!\")\n",
    "    print(f\"   Images with predicted burners: {len(df_with_inference[df_with_inference['has_inferred_burners']])}\")\n",
    "    print(f\"   Total predicted burners: {df_with_inference['num_inferred_burners'].sum()}\")\n",
    "    print(f\"   Average predicted burners per image: {df_with_inference['num_inferred_burners'].mean():.2f}\")\n",
    "    \n",
    "    return df_with_inference\n",
    "\n",
    "def visualize_inference_results(df: pd.DataFrame, num_samples: int = 6):\n",
    "    \"\"\"Visualize inference results\"\"\"\n",
    "    print(f\"\\n📊 VISUALIZING INFERENCE RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Select samples with ground truth burners\n",
    "    samples = df[df['has_burners']].head(num_samples)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows()):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Display image\n",
    "        axes[i].imshow(row['image'])\n",
    "        axes[i].set_title(f\"{row['image_name']}\\nGT: {row['num_burners']}, Pred: {row['num_inferred_burners']}\")\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        h, w = row['image'].shape[:2]\n",
    "        \n",
    "        # Draw ground truth bounding boxes (green)\n",
    "        for bbox in row['burner_bounding_boxes']:\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            x1, y1 = int(xmin * w), int(ymin * h)\n",
    "            x2, y2 = int(xmax * w), int(ymax * h)\n",
    "            \n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, \n",
    "                                   linewidth=2, edgecolor='green', facecolor='none')\n",
    "            axes[i].add_patch(rect)\n",
    "        \n",
    "        # Draw inferred bounding boxes (red)\n",
    "        for bbox in row['inferred_burner_bboxes']:\n",
    "            xmin, ymin, xmax, ymax, confidence = bbox\n",
    "            x1, y1 = int(xmin * w), int(ymin * h)\n",
    "            x2, y2 = int(xmax * w), int(ymax * h)\n",
    "            \n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, \n",
    "                                   linewidth=2, edgecolor='red', facecolor='none', linestyle='--')\n",
    "            axes[i].add_patch(rect)\n",
    "            \n",
    "            # Add confidence score\n",
    "            axes[i].text(x1, y1 - 5, f'{confidence:.2f}', \n",
    "                        color='red', fontsize=8, weight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    import matplotlib.lines as mlines\n",
    "    green_line = mlines.Line2D([], [], color='green', linewidth=2, label='Ground Truth')\n",
    "    red_line = mlines.Line2D([], [], color='red', linewidth=2, linestyle='--', label='Predictions')\n",
    "    plt.legend(handles=[green_line, red_line], loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run inference if model is available\n",
    "if model_interpreter is not None:\n",
    "    print(\"\\n🚀 STEP 4: INFERENCE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    dataset_df = run_inference_on_dataframe(dataset_df, model_interpreter, \"simple\")\n",
    "    \n",
    "    # Visualize results\n",
    "    if not dataset_df.empty:\n",
    "        visualize_inference_results(dataset_df)\n",
    "else:\n",
    "    print(\"\\n⚠️  SKIPPING STEP 4: No model available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71074af",
   "metadata": {},
   "source": [
    "## Advanced Evaluation with IoU Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e564bfe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def convert_bbox_format(bbox, format_type: str) -> List[float]:\n",
    "    \"\"\"Convert between different bounding box formats with robust error handling\"\"\"\n",
    "    if format_type == \"gt_to_pred\":\n",
    "        # Ground truth: {xMinNormalized, yMinNormalized, xMaxNormalized, yMaxNormalized}\n",
    "        # to Model: [ymin, xmin, ymax, xmax]\n",
    "        required_keys = [\"xMinNormalized\", \"yMinNormalized\", \"xMaxNormalized\", \"yMaxNormalized\"]\n",
    "        \n",
    "        # Check if all required keys are present\n",
    "        missing_keys = [key for key in required_keys if key not in bbox]\n",
    "        if missing_keys:\n",
    "            raise KeyError(f\"Missing required keys in bbox: {missing_keys}. Available keys: {list(bbox.keys())}\")\n",
    "        \n",
    "        try:\n",
    "            return [bbox[\"yMinNormalized\"], bbox[\"xMinNormalized\"], bbox[\"yMaxNormalized\"], bbox[\"xMaxNormalized\"]]\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Error accessing bbox coordinates: {e}. Bbox content: {bbox}\")\n",
    "            \n",
    "    elif format_type == \"pred_to_gt\":\n",
    "        # Model: [ymin, xmin, ymax, xmax]\n",
    "        # to Ground truth format: {xMinNormalized, yMinNormalized, xMaxNormalized, yMaxNormalized}\n",
    "        return {\n",
    "            \"xMinNormalized\": bbox[1],\n",
    "            \"yMinNormalized\": bbox[0],\n",
    "            \"xMaxNormalized\": bbox[3],\n",
    "            \"yMaxNormalized\": bbox[2]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown format_type: {format_type}\")\n",
    "\n",
    "def is_valid_bbox(bbox: Dict) -> bool:\n",
    "    \"\"\"Check if a bounding box has all required coordinate keys\"\"\"\n",
    "    required_keys = [\"xMinNormalized\", \"yMinNormalized\", \"xMaxNormalized\", \"yMaxNormalized\"]\n",
    "    return all(key in bbox for key in required_keys)\n",
    "\n",
    "def calculate_iou(box1: List[float], box2: List[float]) -> float:\n",
    "    \"\"\"Calculate IoU between two bounding boxes in [ymin, xmin, ymax, xmax] format\"\"\"\n",
    "    # box1 and box2 should be in format [ymin, xmin, ymax, xmax]\n",
    "    y1_min, x1_min, y1_max, x1_max = box1\n",
    "    y2_min, x2_min, y2_max, x2_max = box2\n",
    "    \n",
    "    # Calculate intersection\n",
    "    x_left = max(x1_min, x2_min)\n",
    "    y_top = max(y1_min, y2_min)\n",
    "    x_right = min(x1_max, x2_max)\n",
    "    y_bottom = min(y1_max, y2_max)\n",
    "    \n",
    "    if x_right <= x_left or y_bottom <= y_top:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x_right - x_left) * (y_bottom - y_top)\n",
    "    \n",
    "    # Calculate union\n",
    "    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def evaluate_with_iou(results: List[Dict], iou_threshold: float = 0.5) -> Dict:\n",
    "    \"\"\"Evaluate predictions using IoU matching\"\"\"\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    detailed_results = []\n",
    "    bbox_errors = []\n",
    "    \n",
    "    for result in results:\n",
    "        if not result[\"inference_success\"]:\n",
    "            continue\n",
    "            \n",
    "        # Get ground truth burner boxes\n",
    "        gt_boxes = []\n",
    "        if 'annotations' in result.get('metadata', {}):\n",
    "            for bbox in result['metadata']['annotations'].get('bboxes', []):\n",
    "                if 'burner' in bbox.get('label', '').lower():\n",
    "                    if is_valid_bbox(bbox):\n",
    "                        gt_boxes.append(convert_bbox_format(bbox, \"gt_to_pred\"))\n",
    "                    else:\n",
    "                        # Log problematic bbox for debugging\n",
    "                        bbox_errors.append({\n",
    "                            'file': result['file'],\n",
    "                            'bbox': bbox,\n",
    "                            'missing_keys': [key for key in [\"xMinNormalized\", \"yMinNormalized\", \"xMaxNormalized\", \"yMaxNormalized\"] if key not in bbox]\n",
    "                        })\n",
    "        \n",
    "        # Get predicted burner boxes\n",
    "        pred_boxes = []\n",
    "        for det in result.get('detections', []):\n",
    "            if det['class_id'] == 0:  # burner class\n",
    "                pred_boxes.append(det['bbox'])\n",
    "        \n",
    "        # Match predictions to ground truth\n",
    "        matched_gt = set()\n",
    "        matched_pred = set()\n",
    "        matches = []\n",
    "        \n",
    "        for pred_idx, pred_box in enumerate(pred_boxes):\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "            \n",
    "            for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                if gt_idx in matched_gt:\n",
    "                    continue\n",
    "                    \n",
    "                iou = calculate_iou(pred_box, gt_box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = gt_idx\n",
    "            \n",
    "            if best_iou >= iou_threshold:\n",
    "                matched_gt.add(best_gt_idx)\n",
    "                matched_pred.add(pred_idx)\n",
    "                matches.append({\n",
    "                    'gt_idx': best_gt_idx,\n",
    "                    'pred_idx': pred_idx,\n",
    "                    'iou': best_iou,\n",
    "                    'confidence': result['detections'][pred_idx]['confidence']\n",
    "                })\n",
    "                true_positives += 1\n",
    "        \n",
    "        # Count false positives and false negatives\n",
    "        false_positives += len(pred_boxes) - len(matched_pred)\n",
    "        false_negatives += len(gt_boxes) - len(matched_gt)\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'file': result['file'],\n",
    "            'gt_boxes': len(gt_boxes),\n",
    "            'pred_boxes': len(pred_boxes),\n",
    "            'matches': matches,\n",
    "            'tp': len(matches),\n",
    "            'fp': len(pred_boxes) - len(matched_pred),\n",
    "            'fn': len(gt_boxes) - len(matched_gt)\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Log bbox errors if any\n",
    "    if bbox_errors:\n",
    "        print(f\"\\n⚠️  Found {len(bbox_errors)} problematic bounding boxes:\")\n",
    "        for error in bbox_errors[:5]:  # Show first 5\n",
    "            print(f\"  File: {error['file']}\")\n",
    "            print(f\"  Missing keys: {error['missing_keys']}\")\n",
    "            print(f\"  Available keys: {list(error['bbox'].keys())}\")\n",
    "            print(f\"  Bbox content: {error['bbox']}\")\n",
    "        if len(bbox_errors) > 5:\n",
    "            print(f\"  ... and {len(bbox_errors) - 5} more\")\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives,\n",
    "        'detailed_results': detailed_results,\n",
    "        'bbox_errors': len(bbox_errors)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6a9059",
   "metadata": {},
   "source": [
    "## Visualization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c20a6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_single_image_pipeline(metadata_file: str, model_path: str):\n",
    "    \"\"\"Demonstrate the pipeline on a single sample image\"\"\"\n",
    "    \n",
    "    print(\"🔍 Loading sample image and metadata...\")\n",
    "    \n",
    "    # Load metadata and find image\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    image_path = find_image_for_metadata(metadata_file)\n",
    "    if not image_path:\n",
    "        print(f\"❌ No image found for {metadata_file}\")\n",
    "        return\n",
    "    \n",
    "    # Load model\n",
    "    interpreter = load_model(model_path)\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Step 1: Show ground truth\n",
    "    print(f\"\\n📋 Step 1: Ground Truth Analysis\")\n",
    "    print(f\"   Image: {os.path.basename(image_path)}\")\n",
    "    print(f\"   Original size: {original_image.size}\")\n",
    "    \n",
    "    all_gt_labels = []\n",
    "    gt_boxes = []\n",
    "    invalid_gt_boxes = 0\n",
    "    if 'annotations' in metadata:\n",
    "        for bbox in metadata['annotations'].get('bboxes', []):\n",
    "            label = bbox.get('label', '')\n",
    "            all_gt_labels.append(label)\n",
    "            if 'burner' in label.lower():\n",
    "                if is_valid_bbox(bbox):\n",
    "                    gt_boxes.append(bbox)\n",
    "                else:\n",
    "                    invalid_gt_boxes += 1\n",
    "    \n",
    "    print(f\"   All ground truth labels: {all_gt_labels}\")\n",
    "    print(f\"   Burner labels found: {len(gt_boxes)} burners\")\n",
    "    if invalid_gt_boxes > 0:\n",
    "        print(f\"   ⚠️  Skipped {invalid_gt_boxes} burner boxes with invalid coordinates\")\n",
    "    \n",
    "    # Step 2: Show preprocessing\n",
    "    print(f\"\\n🔄 Step 2: Preprocessing\")\n",
    "    input_details = interpreter.get_input_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "    target_size = (input_shape[1], input_shape[2])\n",
    "    \n",
    "    print(f\"   Model expects: {target_size} pixels, {input_dtype} data type\")\n",
    "    preprocessed = preprocess_image(image_path, target_size, input_dtype)\n",
    "    print(f\"   Preprocessed shape: {preprocessed.shape}\")\n",
    "    \n",
    "    # Step 3: Show inference results\n",
    "    print(f\"\\n🎯 Step 3: Inference Results\")\n",
    "    detections = run_inference(image_path, interpreter, \"simple\")\n",
    "    pred_boxes = [det for det in detections if det['class_id'] == 0]\n",
    "    \n",
    "    print(f\"   Total detections: {len(detections)}\")\n",
    "    print(f\"   Burner detections: {len(pred_boxes)}\")\n",
    "    \n",
    "    for i, det in enumerate(pred_boxes):\n",
    "        print(f\"     - Burner {i+1}: confidence={det['confidence']:.3f}, bbox={det['bbox']}\")\n",
    "    \n",
    "    # Step 4: Visual comparison\n",
    "    print(f\"\\n📸 Step 4: Creating Visual Comparison\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Original + Ground Truth\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(f\"Ground Truth\\n({len(gt_boxes)} burners)\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    for bbox in gt_boxes:\n",
    "        # Only draw boxes that have valid coordinates\n",
    "        if is_valid_bbox(bbox):\n",
    "            w, h = original_image.size\n",
    "            x_min = int(bbox[\"xMinNormalized\"] * w)\n",
    "            y_min = int(bbox[\"yMinNormalized\"] * h)\n",
    "            x_max = int(bbox[\"xMaxNormalized\"] * w)\n",
    "            y_max = int(bbox[\"yMaxNormalized\"] * h)\n",
    "            \n",
    "            rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                               fill=False, edgecolor='green', linewidth=3)\n",
    "            axes[0].add_patch(rect)\n",
    "            axes[0].text(x_min, y_min - 10, \"GT\", color='green', fontsize=12, weight='bold')\n",
    "    \n",
    "    # Preprocessed image\n",
    "    if input_dtype == np.uint8:\n",
    "        display_image = preprocessed[0].astype(np.uint8)\n",
    "    else:\n",
    "        display_image = (preprocessed[0] * 255).astype(np.uint8)\n",
    "    \n",
    "    axes[1].imshow(display_image)\n",
    "    axes[1].set_title(f\"Preprocessed\\n{target_size}, {input_dtype}\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Original + Predictions\n",
    "    axes[2].imshow(original_image)\n",
    "    axes[2].set_title(f\"Predictions\\n({len(pred_boxes)} burners)\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    for i, det in enumerate(pred_boxes):\n",
    "        w, h = original_image.size\n",
    "        y_min = int(det['bbox'][0] * h)\n",
    "        x_min = int(det['bbox'][1] * w)\n",
    "        y_max = int(det['bbox'][2] * h)\n",
    "        x_max = int(det['bbox'][3] * w)\n",
    "        \n",
    "        rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                           fill=False, edgecolor='red', linewidth=3)\n",
    "        axes[2].add_patch(rect)\n",
    "        axes[2].text(x_min, y_min - 10, f\"{det['confidence']:.2f}\", \n",
    "                    color='red', fontsize=12, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"pipeline_demo.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n✅ Pipeline Demo Complete!\")\n",
    "    print(f\"   📊 Ground Truth: {len(gt_boxes)} burners\")\n",
    "    print(f\"   🎯 Predictions: {len(pred_boxes)} burners\")\n",
    "    print(f\"   📸 Visualization saved: pipeline_demo.png\")\n",
    "    print(f\"   🎪 Demo shows: GT (green) vs Predictions (red)\")\n",
    "    \n",
    "    if len(gt_boxes) > 0 and len(pred_boxes) > 0:\n",
    "        print(f\"   ✅ Model found burners in image with burners!\")\n",
    "    elif len(gt_boxes) > 0 and len(pred_boxes) == 0:\n",
    "        print(f\"   ❌ Model missed burners that should be there\")\n",
    "    elif len(gt_boxes) == 0 and len(pred_boxes) > 0:\n",
    "        print(f\"   ❌ Model detected burners where there are none\")\n",
    "    else:\n",
    "        print(f\"   ✅ Model correctly found no burners in image with no burners\")\n",
    "\n",
    "def process_all_images_with_iou(model_path: str):\n",
    "    \"\"\"Process all images with full metadata for IoU evaluation\"\"\"\n",
    "    if not model_path or not os.path.exists(model_path):\n",
    "        print(\"❌ Model not found\")\n",
    "        return []\n",
    "    \n",
    "    interpreter = load_model(model_path)\n",
    "    metadata_files = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(metadata_files)} images for IoU evaluation...\")\n",
    "    \n",
    "    for i, metadata_file in enumerate(metadata_files):\n",
    "        # Load metadata\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Find corresponding image\n",
    "        image_path = find_image_for_metadata(metadata_file)\n",
    "        if not image_path:\n",
    "            continue\n",
    "        \n",
    "        # Run inference\n",
    "        detections = run_inference(image_path, interpreter, \"simple\")\n",
    "        \n",
    "        results.append({\n",
    "            \"file\": os.path.basename(metadata_file),\n",
    "            \"image_path\": image_path,\n",
    "            \"metadata\": metadata,\n",
    "            \"detections\": detections,\n",
    "            \"inference_success\": True\n",
    "        })\n",
    "        \n",
    "        # Progress update every 500 images\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"  Progress: {i + 1}/{len(metadata_files)} images processed\")\n",
    "    \n",
    "    print(f\"\\n✅ IoU evaluation complete: {len(results)} images processed\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b2ff07",
   "metadata": {},
   "source": [
    "## Working with the Saved DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load and work with your saved dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📖 HOW TO USE YOUR SAVED DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# This is how you would load your dataset in a new session\n",
    "# df = load_dataframe(\"burner_dataset.pkl\")\n",
    "\n",
    "# Example queries you can perform:\n",
    "print(\"\\n💡 Example DataFrame Operations:\")\n",
    "print(\"   # Load your dataset\")\n",
    "print(\"   df = load_dataframe('burner_dataset.pkl')\")\n",
    "print()\n",
    "print(\"   # Get all images with burners\")\n",
    "print(\"   images_with_burners = df[df['num_burners'] > 0]\")\n",
    "print()\n",
    "print(\"   # Get all burner bounding boxes from a specific image\")\n",
    "print(\"   image_name = 'your_image.jpg'\")\n",
    "print(\"   image_row = df[df['image_name'] == image_name].iloc[0]\")\n",
    "print(\"   burner_boxes = image_row['burner_bboxes']\")\n",
    "print()\n",
    "print(\"   # Filter images with multiple burners\")\n",
    "print(\"   multi_burner_images = df[df['num_burners'] > 1]\")\n",
    "print()\n",
    "print(\"   # Get statistics\")\n",
    "print(\"   total_images = len(df)\")\n",
    "print(\"   total_burners = df['num_burners'].sum()\")\n",
    "print(\"   avg_burners_per_image = df['num_burners'].mean()\")\n",
    "print()\n",
    "print(\"   # Export to other formats\")\n",
    "print(\"   df.to_csv('dataset_summary.csv', index=False)\")\n",
    "print(\"   df.to_json('dataset.json', orient='records')\")\n",
    "\n",
    "print(\"\\n✅ Dataset is ready for use!\")\n",
    "print(\"   📁 Files created:\")\n",
    "print(\"     - burner_dataset.pkl (full dataset with bounding boxes)\")\n",
    "print(\"     - burner_dataset_summary.csv (summary statistics)\")\n",
    "if model_path:\n",
    "    print(\"     - burner_dataset_with_predictions.pkl (dataset + model predictions)\")\n",
    "    print(\"     - burner_dataset_with_predictions_summary.csv (prediction summary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da692093",
   "metadata": {},
   "source": [
    "## Evaluation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea676812",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Process all images\n",
    "if model_path:\n",
    "    # ===== EVALUATION METHOD 1: Simple Presence/Absence =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📊 SIMPLE EVALUATION (Presence/Absence)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = process_all_images(model_path)\n",
    "    \n",
    "    # ===== EVALUATION METHOD 2: Advanced IoU-Based =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 ADVANCED EVALUATION (IoU-Based)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    iou_results = process_all_images_with_iou(model_path)\n",
    "    if iou_results:\n",
    "        evaluation = evaluate_with_iou(iou_results, iou_threshold=0.5)\n",
    "        \n",
    "        print(f\"\\n📊 IoU Evaluation Results (threshold=0.5):\")\n",
    "        print(f\"   Precision: {evaluation['precision']:.3f}\")\n",
    "        print(f\"   Recall: {evaluation['recall']:.3f}\")\n",
    "        print(f\"   F1 Score: {evaluation['f1']:.3f}\")\n",
    "        print(f\"   True Positives: {evaluation['true_positives']}\")\n",
    "        print(f\"   False Positives: {evaluation['false_positives']}\")\n",
    "        print(f\"   False Negatives: {evaluation['false_negatives']}\")\n",
    "        \n",
    "        # Show detailed per-image results\n",
    "        print(f\"\\n📋 Per-Image Results:\")\n",
    "        for detail in evaluation['detailed_results'][:5]:  # Show first 5\n",
    "            print(f\"   {detail['file']}: GT={detail['gt_boxes']}, Pred={detail['pred_boxes']}, \"\n",
    "                  f\"TP={detail['tp']}, FP={detail['fp']}, FN={detail['fn']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Skipping processing - no model available\")\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f9f64",
   "metadata": {},
   "source": [
    "## Step 3: Check Burner Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5eeed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def analyze_burner_performance(results: List[Dict]):\n",
    "    \"\"\"Analyze burner detection performance\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    total = len(results)\n",
    "    true_positives = sum(1 for r in results if r['has_burner_gt'] and r['has_burner_pred'])\n",
    "    false_positives = sum(1 for r in results if not r['has_burner_gt'] and r['has_burner_pred'])\n",
    "    false_negatives = sum(1 for r in results if r['has_burner_gt'] and not r['has_burner_pred'])\n",
    "    true_negatives = sum(1 for r in results if not r['has_burner_gt'] and not r['has_burner_pred'])\n",
    "    \n",
    "    accuracy = (true_positives + true_negatives) / total if total > 0 else 0\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    print(\"\\n=== Simple Burner Classification Results ===\")\n",
    "    print(f\"Total images: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Precision: {precision:.2%}\")\n",
    "    print(f\"Recall: {recall:.2%}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  True Positives: {true_positives}\")\n",
    "    print(f\"  False Positives: {false_positives}\")\n",
    "    print(f\"  False Negatives: {false_negatives}\")\n",
    "    print(f\"  True Negatives: {true_negatives}\")\n",
    "    \n",
    "    # Show misclassified examples\n",
    "    print(f\"\\nMisclassifications:\")\n",
    "    for result in results:\n",
    "        if result['has_burner_gt'] != result['has_burner_pred']:\n",
    "            status = \"Missing burner\" if result['has_burner_gt'] else \"False detection\"\n",
    "            print(f\"  {result['file']}: {status}\")\n",
    "\n",
    "# Analyze results\n",
    "analyze_burner_performance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2f8af",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87f966",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "if results:\n",
    "    output_file = \"burner_classification_results.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\n📄 Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211ccf6",
   "metadata": {},
   "source": [
    "## Preprocessing Method Comparison\n",
    "\n",
    "Compare different normalization techniques to handle lighting variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaaaabd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_images_with_preprocessing_method(model_path: str, normalization_method: str, \n",
    "                                           max_images: int = 100) -> List[Dict]:\n",
    "    \"\"\"Process subset of images with specific preprocessing method\"\"\"\n",
    "    if not model_path or not os.path.exists(model_path):\n",
    "        print(f\"❌ Model not found\")\n",
    "        return []\n",
    "    \n",
    "    interpreter = load_model(model_path)\n",
    "    metadata_files = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "    \n",
    "    # Limit to subset for comparison (processing all 8000+ would take too long)\n",
    "    test_files = metadata_files[:max_images]\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Testing {normalization_method} normalization on {len(test_files)} images...\")\n",
    "    \n",
    "    for i, metadata_file in enumerate(test_files):\n",
    "        # Load metadata\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        # Find corresponding image\n",
    "        image_path = find_image_for_metadata(metadata_file)\n",
    "        if not image_path:\n",
    "            continue\n",
    "        \n",
    "        # Run inference with specific normalization method\n",
    "        detections = run_inference(image_path, interpreter, normalization_method)\n",
    "        \n",
    "        # Extract ground truth burner labels\n",
    "        gt_burners = []\n",
    "        if 'annotations' in metadata:\n",
    "            for bbox in metadata['annotations'].get('bboxes', []):\n",
    "                if 'burner' in bbox.get('label', '').lower():\n",
    "                    gt_burners.append(bbox['label'])\n",
    "        \n",
    "        # Check if model detected burners\n",
    "        pred_burners = [det for det in detections if det['class_id'] == 0]\n",
    "        \n",
    "        results.append({\n",
    "            \"file\": os.path.basename(metadata_file),\n",
    "            \"image_path\": image_path,\n",
    "            \"ground_truth_burners\": len(gt_burners),\n",
    "            \"predicted_burners\": len(pred_burners),\n",
    "            \"detections\": detections,\n",
    "            \"has_burner_gt\": len(gt_burners) > 0,\n",
    "            \"has_burner_pred\": len(pred_burners) > 0,\n",
    "            \"normalization_method\": normalization_method\n",
    "        })\n",
    "        \n",
    "        # Progress update every 25 images for smaller batches\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f\"  Progress: {i + 1}/{len(test_files)} images processed\")\n",
    "    \n",
    "    print(f\"✅ {normalization_method} processing complete: {len(results)} images\")\n",
    "    return results\n",
    "\n",
    "def compare_preprocessing_methods(model_path: str, max_images: int = 100):\n",
    "    \"\"\"Compare all three preprocessing methods\"\"\"\n",
    "    methods = [\"simple\", \"gcn\", \"lcn\"]\n",
    "    all_results = {}\n",
    "    \n",
    "    print(\"🧪 PREPROCESSING METHOD COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Testing {len(methods)} normalization methods on {max_images} images each\")\n",
    "    print(\"Methods: Simple (0-1), GCN (Global Contrast), LCN (Local Contrast)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test each method\n",
    "    for method in methods:\n",
    "        print(f\"\\n🔬 Testing {method.upper()} normalization...\")\n",
    "        results = process_images_with_preprocessing_method(model_path, method, max_images)\n",
    "        all_results[method] = results\n",
    "    \n",
    "    # Compare results\n",
    "    print(f\"\\n📊 PREPROCESSING COMPARISON RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for method, results in all_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        total = len(results)\n",
    "        true_positives = sum(1 for r in results if r['has_burner_gt'] and r['has_burner_pred'])\n",
    "        false_positives = sum(1 for r in results if not r['has_burner_gt'] and r['has_burner_pred'])\n",
    "        false_negatives = sum(1 for r in results if r['has_burner_gt'] and not r['has_burner_pred'])\n",
    "        true_negatives = sum(1 for r in results if not r['has_burner_gt'] and not r['has_burner_pred'])\n",
    "        \n",
    "        accuracy = (true_positives + true_negatives) / total if total > 0 else 0\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Count total detections (regardless of correctness)\n",
    "        total_detections = sum(r['predicted_burners'] for r in results)\n",
    "        avg_detections = total_detections / total if total > 0 else 0\n",
    "        \n",
    "        print(f\"\\n🔸 {method.upper()} Normalization:\")\n",
    "        print(f\"   Accuracy:  {accuracy:.3f}\")\n",
    "        print(f\"   Precision: {precision:.3f}\")\n",
    "        print(f\"   Recall:    {recall:.3f}\")\n",
    "        print(f\"   F1 Score:  {f1:.3f}\")\n",
    "        print(f\"   Avg Detections/Image: {avg_detections:.2f}\")\n",
    "        print(f\"   TP: {true_positives}, FP: {false_positives}, FN: {false_negatives}, TN: {true_negatives}\")\n",
    "    \n",
    "    # Find best method\n",
    "    best_method = None\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for method, results in all_results.items():\n",
    "        if not results:\n",
    "            continue\n",
    "        total = len(results)\n",
    "        true_positives = sum(1 for r in results if r['has_burner_gt'] and r['has_burner_pred'])\n",
    "        false_positives = sum(1 for r in results if not r['has_burner_gt'] and r['has_burner_pred'])\n",
    "        false_negatives = sum(1 for r in results if r['has_burner_gt'] and not r['has_burner_pred'])\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_method = method\n",
    "    \n",
    "    if best_method:\n",
    "        print(f\"\\n🏆 BEST PREPROCESSING METHOD: {best_method.upper()}\")\n",
    "        print(f\"   Best F1 Score: {best_f1:.3f}\")\n",
    "        print(f\"   💡 Recommended for production use!\")\n",
    "    \n",
    "    # Save comparison results\n",
    "    comparison_file = \"preprocessing_comparison.json\"\n",
    "    with open(comparison_file, 'w') as f:\n",
    "        # Convert results to JSON-serializable format\n",
    "        serializable_results = {}\n",
    "        for method, results in all_results.items():\n",
    "            serializable_results[method] = []\n",
    "            for result in results:\n",
    "                # Remove non-serializable items\n",
    "                clean_result = {k: v for k, v in result.items() if k != 'detections'}\n",
    "                clean_result['detection_count'] = len(result.get('detections', []))\n",
    "                serializable_results[method].append(clean_result)\n",
    "        \n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n📄 Comparison results saved to {comparison_file}\")\n",
    "    return all_results\n",
    "\n",
    "def visualize_preprocessing_effects(metadata_file: str, model_path: str):\n",
    "    \"\"\"Visualize the effects of different preprocessing methods on a single image\"\"\"\n",
    "    \n",
    "    print(\"🔍 Visualizing preprocessing effects on sample image...\")\n",
    "    \n",
    "    # Load metadata and find image\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    image_path = find_image_for_metadata(metadata_file)\n",
    "    if not image_path:\n",
    "        print(f\"❌ No image found for {metadata_file}\")\n",
    "        return\n",
    "    \n",
    "    # Load original image\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Test all three preprocessing methods\n",
    "    methods = [\"simple\", \"gcn\", \"lcn\"]\n",
    "    method_names = [\"Simple (0-1)\", \"Global Contrast Norm\", \"Local Contrast Norm\"]\n",
    "    \n",
    "    # Load model to get expected input format\n",
    "    interpreter = load_model(model_path)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_dtype = input_details[0]['dtype']\n",
    "    target_size = (input_shape[1], input_shape[2])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Apply each preprocessing method\n",
    "    for i, (method, name) in enumerate(zip(methods, method_names)):\n",
    "        preprocessed = preprocess_image(image_path, target_size, input_dtype, method)\n",
    "        \n",
    "        # Convert back to display format\n",
    "        if input_dtype == np.uint8:\n",
    "            display_image = preprocessed[0].astype(np.uint8)\n",
    "        else:\n",
    "            display_image = (preprocessed[0] * 255).astype(np.uint8)\n",
    "        \n",
    "        axes[i + 1].imshow(display_image)\n",
    "        axes[i + 1].set_title(f\"{name}\")\n",
    "        axes[i + 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"preprocessing_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"📸 Preprocessing comparison saved: preprocessing_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632e513",
   "metadata": {},
   "source": [
    "## Pipeline Visualization Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3501d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ===== PIPELINE DEMO: Visual Walkthrough =====\n",
    "if model_path:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"📸 PIPELINE VISUALIZATION DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Visualize one sample image to demonstrate the pipeline\n",
    "    metadata_files = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "    if metadata_files:\n",
    "        sample_metadata = metadata_files[0]  # Just take the first one\n",
    "        print(f\"Demonstrating pipeline on sample image: {os.path.basename(sample_metadata)}\")\n",
    "        visualize_single_image_pipeline(sample_metadata, model_path)\n",
    "    else:\n",
    "        print(\"⚠️  No metadata files found for visualization demo\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping visualization demo - no model available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298c2a2",
   "metadata": {},
   "source": [
    "## Debugging Simple Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_simple_evaluation(results: List[Dict], sample_limit: int = 10):\n",
    "    \"\"\"\n",
    "    Debug why simple evaluation is getting 0 true positives\n",
    "    \"\"\"\n",
    "    print(\"🔍 DEBUGGING SIMPLE EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"❌ No results to debug!\")\n",
    "        return\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_images = len(results)\n",
    "    has_gt_burners = sum(1 for r in results if r['has_burner_gt'])\n",
    "    has_pred_burners = sum(1 for r in results if r['has_burner_pred'])\n",
    "    true_positives = sum(1 for r in results if r['has_burner_gt'] and r['has_burner_pred'])\n",
    "    \n",
    "    print(f\"📊 OVERALL STATISTICS:\")\n",
    "    print(f\"   Total images: {total_images}\")\n",
    "    print(f\"   Images with GT burners: {has_gt_burners}\")\n",
    "    print(f\"   Images with predicted burners: {has_pred_burners}\")\n",
    "    print(f\"   True positives: {true_positives}\")\n",
    "    \n",
    "    # Sample detailed analysis\n",
    "    print(f\"\\n🔍 DETAILED SAMPLE ANALYSIS (first {sample_limit} images):\")\n",
    "    \n",
    "    gt_label_examples = set()\n",
    "    pred_class_examples = set()\n",
    "    inference_issues = 0\n",
    "    \n",
    "    for i, result in enumerate(results[:sample_limit]):\n",
    "        print(f\"\\n📸 Sample {i+1}: {result['file']}\")\n",
    "        \n",
    "        # Check ground truth\n",
    "        print(f\"   Ground Truth Analysis:\")\n",
    "        all_gt_labels = result.get('all_gt_labels', [])\n",
    "        gt_burner_count = result.get('ground_truth_burners', 0)\n",
    "        \n",
    "        if all_gt_labels:\n",
    "            print(f\"     All labels: {all_gt_labels}\")\n",
    "            gt_label_examples.update(all_gt_labels)\n",
    "        else:\n",
    "            print(f\"     No labels found in annotations\")\n",
    "        \n",
    "        print(f\"     Burner count: {gt_burner_count}\")\n",
    "        print(f\"     has_burner_gt: {result['has_burner_gt']}\")\n",
    "        \n",
    "        # Check predictions\n",
    "        print(f\"   Prediction Analysis:\")\n",
    "        detections = result.get('detections', [])\n",
    "        pred_burner_count = result.get('predicted_burners', 0)\n",
    "        \n",
    "        if detections:\n",
    "            print(f\"     Total detections: {len(detections)}\")\n",
    "            for j, det in enumerate(detections):\n",
    "                class_id = det['class_id']\n",
    "                confidence = det['confidence']\n",
    "                print(f\"       Detection {j+1}: class_id={class_id}, confidence={confidence:.3f}\")\n",
    "                pred_class_examples.add(class_id)\n",
    "        else:\n",
    "            print(f\"     No detections found\")\n",
    "            inference_issues += 1\n",
    "        \n",
    "        print(f\"     Burner detections (class_id=0): {pred_burner_count}\")\n",
    "        print(f\"     has_burner_pred: {result['has_burner_pred']}\")\n",
    "        \n",
    "        # Check if this should be a true positive\n",
    "        if result['has_burner_gt'] and result['has_burner_pred']:\n",
    "            print(f\"     ✅ This is a TRUE POSITIVE\")\n",
    "        elif result['has_burner_gt'] and not result['has_burner_pred']:\n",
    "            print(f\"     ❌ This is a FALSE NEGATIVE (GT has burners, model doesn't)\")\n",
    "        elif not result['has_burner_gt'] and result['has_burner_pred']:\n",
    "            print(f\"     ❌ This is a FALSE POSITIVE (model detects burners, GT doesn't)\")\n",
    "        else:\n",
    "            print(f\"     ✅ This is a TRUE NEGATIVE (no burners in GT or predictions)\")\n",
    "    \n",
    "    print(f\"\\n📋 SUMMARY OF FINDINGS:\")\n",
    "    print(f\"   Unique GT labels seen: {sorted(gt_label_examples)}\")\n",
    "    print(f\"   Unique prediction class_ids seen: {sorted(pred_class_examples)}\")\n",
    "    print(f\"   Images with no detections: {inference_issues}\")\n",
    "    \n",
    "    # Diagnose the issue\n",
    "    print(f\"\\n🔧 DIAGNOSIS:\")\n",
    "    if has_gt_burners == 0:\n",
    "        print(\"   ❌ ISSUE: No ground truth burners found!\")\n",
    "        print(\"      - Check if GT labels contain 'burner' (case-insensitive)\")\n",
    "        print(\"      - Check if metadata files have 'annotations' field\")\n",
    "    elif has_pred_burners == 0:\n",
    "        print(\"   ❌ ISSUE: No predicted burners found!\")\n",
    "        print(\"      - Check if model is producing any detections\")\n",
    "        print(\"      - Check if burner class_id should be 0\")\n",
    "        print(\"      - Check confidence thresholds\")\n",
    "    else:\n",
    "        print(\"   ❌ ISSUE: GT and predictions don't align!\")\n",
    "        print(\"      - Check if burner labels in GT match detection logic\")\n",
    "        print(\"      - Check if there are images with both GT and pred burners\")\n",
    "    \n",
    "    return {\n",
    "        'total_images': total_images,\n",
    "        'has_gt_burners': has_gt_burners,\n",
    "        'has_pred_burners': has_pred_burners,\n",
    "        'true_positives': true_positives,\n",
    "        'gt_labels': gt_label_examples,\n",
    "        'pred_classes': pred_class_examples,\n",
    "        'inference_issues': inference_issues\n",
    "    }\n",
    "\n",
    "def inspect_raw_data(sample_limit: int = 3):\n",
    "    \"\"\"\n",
    "    Inspect raw ground truth and model output formats\n",
    "    \"\"\"\n",
    "    print(\"🔍 INSPECTING RAW DATA FORMATS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check ground truth format\n",
    "    metadata_files = glob.glob(os.path.join(METADATA_DIR, \"*.json\"))\n",
    "    if not metadata_files:\n",
    "        print(\"❌ No metadata files found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📋 GROUND TRUTH INSPECTION:\")\n",
    "    for i, metadata_file in enumerate(metadata_files[:sample_limit]):\n",
    "        print(f\"\\n📸 Sample {i+1}: {os.path.basename(metadata_file)}\")\n",
    "        \n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"   Available keys: {list(metadata.keys())}\")\n",
    "        \n",
    "        if 'annotations' in metadata:\n",
    "            annotations = metadata['annotations']\n",
    "            print(f\"   Annotations keys: {list(annotations.keys())}\")\n",
    "            \n",
    "            if 'bboxes' in annotations:\n",
    "                bboxes = annotations['bboxes']\n",
    "                print(f\"   Number of bboxes: {len(bboxes)}\")\n",
    "                \n",
    "                for j, bbox in enumerate(bboxes[:3]):  # Show first 3 bboxes\n",
    "                    print(f\"     Bbox {j+1}: {bbox}\")\n",
    "            else:\n",
    "                print(\"   No 'bboxes' in annotations\")\n",
    "        else:\n",
    "            print(\"   No 'annotations' in metadata\")\n",
    "    \n",
    "    # Check model output format if we have a model\n",
    "    if model_path and os.path.exists(model_path):\n",
    "        print(f\"\\n🎯 MODEL OUTPUT INSPECTION:\")\n",
    "        \n",
    "        # Find a sample image\n",
    "        sample_metadata = metadata_files[0]\n",
    "        image_path = find_image_for_metadata(sample_metadata)\n",
    "        \n",
    "        if image_path:\n",
    "            print(f\"   Testing with image: {os.path.basename(image_path)}\")\n",
    "            \n",
    "            interpreter = load_model(model_path)\n",
    "            \n",
    "            # Check model input/output details\n",
    "            input_details = interpreter.get_input_details()\n",
    "            output_details = interpreter.get_output_details()\n",
    "            \n",
    "            print(f\"   Model input details:\")\n",
    "            for detail in input_details:\n",
    "                print(f\"     {detail['name']}: shape={detail['shape']}, dtype={detail['dtype']}\")\n",
    "            \n",
    "            print(f\"   Model output details:\")\n",
    "            for detail in output_details:\n",
    "                print(f\"     {detail['name']}: shape={detail['shape']}, dtype={detail['dtype']}\")\n",
    "            \n",
    "            # Run inference and check raw outputs\n",
    "            detections = run_inference(image_path, interpreter, \"simple\")\n",
    "            print(f\"   Raw detections returned: {len(detections)}\")\n",
    "            \n",
    "            for j, det in enumerate(detections[:3]):  # Show first 3 detections\n",
    "                print(f\"     Detection {j+1}: {det}\")\n",
    "        else:\n",
    "            print(\"   No corresponding image found for sample metadata\")\n",
    "    else:\n",
    "        print(\"   No model available for inspection\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
